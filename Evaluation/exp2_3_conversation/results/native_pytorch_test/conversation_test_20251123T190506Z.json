{
  "baseline": "test",
  "model_id": "distilgpt2",
  "device": "cpu",
  "dtype": "float32",
  "runs": [
    {
      "run_id": 1,
      "total_time_ms": 6890.552657656372,
      "turns": [
        {
          "turn_id": 1,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant:",
          "response": null,
          "latency_ms": 887.054187245667,
          "per_token_ms": 6.930110837856773,
          "tokens": 128,
          "host_to_device_mb": 0.0003204345703125,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 2,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant:",
          "response": null,
          "latency_ms": 919.5526912808418,
          "per_token_ms": 7.184005400631577,
          "tokens": 128,
          "host_to_device_mb": 0.00145721435546875,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 3,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant:",
          "response": null,
          "latency_ms": 952.6328826323152,
          "per_token_ms": 7.442444395564962,
          "tokens": 128,
          "host_to_device_mb": 0.0025634765625,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 4,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant:",
          "response": null,
          "latency_ms": 992.5246080383658,
          "per_token_ms": 7.754098500299733,
          "tokens": 128,
          "host_to_device_mb": 0.003692626953125,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 5,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Explain phase detection like I'm new to disaggregation.\n\nAssistant:",
          "response": null,
          "latency_ms": 1042.9910374805331,
          "per_token_ms": 8.148367480316665,
          "tokens": 128,
          "host_to_device_mb": 0.00481414794921875,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 6,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Explain phase detection like I'm new to disaggregation.\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What's the biggest bottleneck you see today?\n\nAssistant:",
          "response": null,
          "latency_ms": 1067.8870677947998,
          "per_token_ms": 8.342867717146873,
          "tokens": 128,
          "host_to_device_mb": 0.00592041015625,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 7,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Explain phase detection like I'm new to disaggregation.\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What's the biggest bottleneck you see today?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: How does QoS keep realtime sessions responsive?\n\nAssistant:",
          "response": null,
          "latency_ms": 892.5688154995441,
          "per_token_ms": 8.925688154995441,
          "tokens": 100,
          "host_to_device_mb": 0.0070343017578125,
          "device_to_host_mb": 0.000762939453125
        },
        {
          "turn_id": 8,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Explain phase detection like I'm new to disaggregation.\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What's the biggest bottleneck you see today?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: How does QoS keep realtime sessions responsive?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I\n\nUser: Wrap it up with a quick summary for our exec team.\n\nAssistant:",
          "response": null,
          "latency_ms": 123.02561104297638,
          "per_token_ms": 123.02561104297638,
          "tokens": 1,
          "host_to_device_mb": 0.0078125,
          "device_to_host_mb": 7.62939453125e-06
        }
      ],
      "aggregates": {
        "turn_latency_ms": {
          "mean": 859.7796126268804,
          "median": 936.0927869565785,
          "p95": 1059.1734571848065,
          "min": 123.02561104297638,
          "max": 1067.8870677947998
        },
        "turn_per_token_ms": {
          "mean": 22.21914919122355,
          "median": 7.951232990308199,
          "p95": 83.09063803218298,
          "min": 6.930110837856773,
          "max": 123.02561104297638
        },
        "total_host_to_device_mb": 0.0336151123046875,
        "total_device_to_host_mb": 0.00662994384765625
      },
      "gpu": {
        "gpu_util_pct": null,
        "mem_util_pct": null,
        "mem_used_mb": null
      }
    }
  ],
  "conversation_file": "Evaluation/exp2_3_conversation/prompts/conversation.json",
  "library_versions": {
    "torch": "2.9.0+cu128",
    "transformers": "4.57.1"
  }
}