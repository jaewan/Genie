{
  "baseline": "native_pytorch_smoke",
  "model_id": "distilgpt2",
  "device": "cpu",
  "dtype": "float32",
  "runs": [
    {
      "run_id": 1,
      "total_time_ms": 7052.78748832643,
      "turns": [
        {
          "turn_id": 1,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant:",
          "response": "I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm",
          "latency_ms": 923.1054680421948,
          "per_token_ms": 7.211761469079647,
          "tokens": 128,
          "host_to_device_mb": 0.0003204345703125,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 2,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant:",
          "response": "I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep",
          "latency_ms": 954.6414455398917,
          "per_token_ms": 7.458136293280404,
          "tokens": 128,
          "host_to_device_mb": 0.00145721435546875,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 3,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant:",
          "response": "I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep",
          "latency_ms": 984.0789968147874,
          "per_token_ms": 7.6881171626155265,
          "tokens": 128,
          "host_to_device_mb": 0.0025634765625,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 4,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant:",
          "response": "I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep",
          "latency_ms": 1021.2283143773675,
          "per_token_ms": 7.978346206073184,
          "tokens": 128,
          "host_to_device_mb": 0.003692626953125,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 5,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Explain phase detection like I'm new to disaggregation.\n\nAssistant:",
          "response": "I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep",
          "latency_ms": 1052.3066157475114,
          "per_token_ms": 8.221145435527433,
          "tokens": 128,
          "host_to_device_mb": 0.00481414794921875,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 6,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Explain phase detection like I'm new to disaggregation.\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What's the biggest bottleneck you see today?\n\nAssistant:",
          "response": "I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep",
          "latency_ms": 1066.2427628412843,
          "per_token_ms": 8.330021584697533,
          "tokens": 128,
          "host_to_device_mb": 0.00592041015625,
          "device_to_host_mb": 0.0009765625
        },
        {
          "turn_id": 7,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Explain phase detection like I'm new to disaggregation.\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What's the biggest bottleneck you see today?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: How does QoS keep realtime sessions responsive?\n\nAssistant:",
          "response": "I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I",
          "latency_ms": 916.485340334475,
          "per_token_ms": 9.16485340334475,
          "tokens": 100,
          "host_to_device_mb": 0.0070343017578125,
          "device_to_host_mb": 0.000762939453125
        },
        {
          "turn_id": 8,
          "prompt": "[System] You are Djinn, a semantic-aware tensor OS engineer.\n\nUser: Hey Djinn, can you remind me what makes your GPU time-sharing different?\n\nAssistant: I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm a software engineer. I'm\n\nUser: How do you keep KV caches around between conversational turns?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What happens if another tenant interrupts my session?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Does the VMU help with streaming audio workloads too?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: Explain phase detection like I'm new to disaggregation.\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: What's the biggest bottleneck you see today?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep\n\nUser: How does QoS keep realtime sessions responsive?\n\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I keep KV caches around between conversational turns.\nAssistant: I\n\nUser: Wrap it up with a quick summary for our exec team.\n\nAssistant:",
          "response": "",
          "latency_ms": 122.40382004529238,
          "per_token_ms": 122.40382004529238,
          "tokens": 1,
          "host_to_device_mb": 0.0078125,
          "device_to_host_mb": 7.62939453125e-06
        }
      ],
      "aggregates": {
        "turn_latency_ms": {
          "mean": 880.0615954678506,
          "median": 969.3602211773396,
          "p95": 1061.3651113584638,
          "min": 122.40382004529238,
          "max": 1066.2427628412843
        },
        "turn_per_token_ms": {
          "mean": 22.307025199988857,
          "median": 8.099745820800308,
          "p95": 82.77018172061064,
          "min": 7.211761469079647,
          "max": 122.40382004529238
        },
        "total_host_to_device_mb": 0.0336151123046875,
        "total_device_to_host_mb": 0.00662994384765625
      },
      "gpu": {
        "gpu_util_pct": null,
        "mem_util_pct": null,
        "mem_used_mb": null
      }
    }
  ],
  "conversation_file": "Evaluation/exp2_3_conversation/prompts/conversation.json",
  "library_versions": {
    "torch": "2.9.0+cu128",
    "transformers": "4.57.1"
  }
}