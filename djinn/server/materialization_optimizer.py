"""
Week 3: Materialization Optimization

Problem: Materialization takes 75ms (71% of overhead)
Root causes:
  - Multiple GPU sync points in recursive execution
  - No batching of operations
  - Individual operation overhead in Python layer
  - Inefficient CPU↔GPU transfers

Solution: Optimized materialization pipeline with:
  1. Topological sort for batch execution
  2. CUDA streams for pipelining
  3. Pinned memory for transfers
  4. Reduced Python overhead
"""

import torch
import torch.cuda
from typing import Dict, List, Optional, Tuple, Set, Any
from dataclasses import dataclass
import logging

logger = logging.getLogger(__name__)


@dataclass
class OperationSchedule:
    """Schedule for optimized execution"""
    operation_id: int
    operation_name: str
    input_ids: List[int]
    output_id: int
    kwargs: Dict[str, Any] = None
    can_fuse_with_next: bool = False

    def __post_init__(self):
        if self.kwargs is None:
            self.kwargs = {}


class MaterializationOptimizer:
    """Optimizes LazyTensor materialization"""
    
    def __init__(self, enable_pinned_memory=True, enable_streams=True):
        self.enable_pinned_memory = enable_pinned_memory
        self.enable_streams = enable_streams
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # CUDA streams for pipelining
        if enable_streams and self.device.type == 'cuda':
            self.compute_stream = torch.cuda.Stream()
            self.transfer_stream = torch.cuda.Stream()
        else:
            self.compute_stream = None
            self.transfer_stream = None
    
    def build_schedule(self, root_lazy_tensor) -> List[OperationSchedule]:
        """
        Build optimal execution schedule using topological sort.
        
        This converts recursive graph traversal into linear schedule
        that can be executed more efficiently.
        """
        schedule = []
        visited = set()
        op_counter = [0]  # Counter for operation IDs
        
        def build_schedule_recursive(lt) -> int:
            """Build schedule recursively, return operation ID"""
            lt_id = id(lt)
            
            if lt_id in visited:
                logger.debug(f"Already visited {lt.operation} (id={lt_id}), returning ID")
                return lt_id
            
            visited.add(lt_id)
            logger.debug(f"Building schedule for {lt.operation} (id={lt_id}), inputs={len(lt.inputs)}")
            
            # Schedule inputs first (DFS post-order)
            input_ids = []
            for i, inp in enumerate(lt.inputs):
                from ..frontend.core.lazy_tensor import LazyTensor
                if isinstance(inp, LazyTensor):
                    inp_id = build_schedule_recursive(inp)
                    input_ids.append(inp_id)
                    logger.debug(f"  Input {i}: LazyTensor {inp.operation} -> ID {inp_id}")
                else:
                    # Concrete input - register it
                    inp_id = id(inp)
                    input_ids.append(inp_id)
                    logger.debug(f"  Input {i}: Concrete -> ID {inp_id}")
            
            # Schedule this operation
            op_id = op_counter[0]
            op_counter[0] += 1
            
            logger.debug(f"Scheduling {lt.operation}: op_id={op_id}, input_ids={input_ids}, output_id={lt_id}")
            
            schedule_entry = OperationSchedule(
                operation_id=op_id,
                operation_name=lt.operation,
                input_ids=input_ids,
                output_id=lt_id,
                kwargs=lt.kwargs if hasattr(lt, 'kwargs') else {},
                can_fuse_with_next=self._can_fuse(lt)
            )
            schedule.append(schedule_entry)
            
            return lt_id
        
        build_schedule_recursive(root_lazy_tensor)
        return schedule
    
    def _can_fuse(self, lt) -> bool:
        """Check if operation can be fused with next for efficiency"""
        # Element-wise ops are fusible
        fusible_ops = {
            'aten::relu', 'aten::sigmoid', 'aten::tanh',
            'aten::neg', 'aten::abs', 'aten::exp', 'aten::log'
        }
        return lt.operation in fusible_ops
    
    def execute_optimized(self, root_lazy_tensor, executor) -> torch.Tensor:
        """
        Execute using optimized schedule.
        
        Key optimizations:
        1. Build topological schedule (avoid repeated traversal)
        2. Use CUDA streams (overlap compute and transfer)
        3. Pinned memory for transfers (faster CPU↔GPU)
        4. Batch operations (reduce sync overhead)
        """
        # Build schedule
        schedule = self.build_schedule(root_lazy_tensor)
        logger.info(f"Optimized schedule: {len(schedule)} operations")
        
        # Prepare execution environment
        result_cache: Dict[int, torch.Tensor] = {}
        concrete_inputs: Dict[int, torch.Tensor] = {}
        
        # Map concrete inputs to IDs - include all non-LazyTensor inputs from the entire graph
        def collect_concrete_inputs(lt, visited):
            from ..frontend.core.lazy_tensor import LazyTensor
            if id(lt) in visited:
                return
            visited.add(id(lt))

            for inp in lt.inputs:
                if isinstance(inp, LazyTensor):
                    collect_concrete_inputs(inp, visited)
                else:
                    concrete_inputs[id(inp)] = inp

        visited = set()
        collect_concrete_inputs(root_lazy_tensor, visited)
        
        # Execute schedule in order
        if self.enable_streams and self.device.type == 'cuda':
            return self._execute_with_streams(schedule, executor, result_cache, concrete_inputs)
        else:
            return self._execute_sequential(schedule, executor, result_cache, concrete_inputs)
    
    def _execute_sequential(self, schedule: List[OperationSchedule],
                           executor, result_cache: Dict[int, torch.Tensor],
                           concrete_inputs: Dict[int, torch.Tensor]) -> torch.Tensor:
        """Execute schedule sequentially without batch executor (avoids recursion)."""
        from ..frontend.core.interception_control import disable_interception, InterceptionContext
        from ..frontend.core.lazy_tensor import LazyTensor
        
        # ✅ FIX: Disable interception for entire execution
        with disable_interception(InterceptionContext.MATERIALIZATION):
            for entry in schedule:
                # Resolve inputs from cache or concrete inputs
                resolved_inputs = []
                for inp_id in entry.input_ids:
                    if inp_id in result_cache:
                        resolved_inputs.append(result_cache[inp_id])
                    elif inp_id in concrete_inputs:
                        resolved_inputs.append(concrete_inputs[inp_id])
                    else:
                        # This input must be from a previous operation
                        logger.warning(f"Missing input {inp_id} for {entry.operation_name}")
                        continue

            # ✅ FIX: Should never have LazyTensors here with interception disabled
            materialized_inputs = []
            for inp in resolved_inputs:
                if isinstance(inp, LazyTensor):
                    raise AssertionError(
                        f"BUG: LazyTensor in resolved_inputs during materialization\n"
                        f"  Operation: {entry.operation_name}\n"
                        f"  Input: {inp.operation if hasattr(inp, 'operation') else 'unknown'}\n"
                        f"  This should not happen with interception disabled.\n"
                    )
                materialized_inputs.append(inp)
            
            # Execute operation directly using executor's handlers or universal dispatcher
            op_handler = executor.operation_handlers.get(entry.operation_name)
            if op_handler:
                # Create minimal context for handler
                class MinimalLT:
                    def __init__(self, operation, kwargs):
                        self.operation = operation
                        self.kwargs = kwargs

                fake_lt = MinimalLT(entry.operation_name, entry.kwargs)
                result = op_handler(fake_lt, materialized_inputs, entry.kwargs)
            else:
                # Use universal dispatcher directly
                from ..frontend.core.universal_dispatcher import get_universal_dispatcher
                dispatcher = get_universal_dispatcher()
                result = dispatcher.dispatch(entry.operation_name, materialized_inputs, entry.kwargs)

            # ✅ FIX: Verify result is concrete
            if isinstance(result, LazyTensor):
                raise AssertionError(
                    f"BUG: Operation returned LazyTensor during materialization\n"
                    f"  Operation: {entry.operation_name}\n"
                    f"  Handler: {op_handler.__name__ if op_handler else 'fallback'}\n"
                    f"  This indicates interception is not properly disabled.\n"
                )
            
            # Cache the result
            result_cache[entry.output_id] = result

        # Return the final result (last operation's output)
        if schedule:
            final_output_id = schedule[-1].output_id
            final_result = result_cache.get(final_output_id, torch.tensor(0.0))
            
            # ✅ FIX: Final sanity check
            if isinstance(final_result, LazyTensor):
                raise AssertionError(
                    f"BUG: Final result is LazyTensor\n"
                    f"  This should be impossible.\n"
                )
            
            return final_result
        else:
            return torch.tensor(0.0)
    
    def _execute_with_streams(self, schedule: List[OperationSchedule],
                              executor, result_cache: Dict[int, torch.Tensor],
                              concrete_inputs: Dict[int, torch.Tensor]) -> torch.Tensor:
        """Execute schedule using CUDA streams for pipelining"""
        from ..frontend.core.lazy_tensor import LazyTensor
        from ..frontend.core.interception_control import disable_interception, InterceptionContext
        
        # ✅ FIX: Disable interception for entire materialization
        with disable_interception(InterceptionContext.MATERIALIZATION):
            # Use streams to overlap computation and transfer
            with torch.cuda.stream(self.compute_stream):
                for entry in schedule:
                    # Resolve inputs
                    resolved_inputs = []
                    logger.debug(f"Executing {entry.operation_name}: looking for {len(entry.input_ids)} inputs")
                    for inp_id in entry.input_ids:
                        if inp_id in result_cache:
                            tensor = result_cache[inp_id]
                            logger.debug(f"  Found input {inp_id} in result_cache: shape={tensor.shape if hasattr(tensor, 'shape') else 'N/A'}")
                            
                            # ✅ FIX: Should NEVER be LazyTensor with interception disabled
                            if isinstance(tensor, LazyTensor):
                                raise AssertionError(
                                    f"BUG: LazyTensor in result_cache during materialization\n"
                                    f"  Operation: {entry.operation_name}\n"
                                    f"  Input ID: {inp_id}\n"
                                    f"  Tensor operation: {tensor.operation}\n"
                                    f"  This should be impossible with interception disabled.\n"
                                )
                            
                            resolved_inputs.append(tensor)
                        elif inp_id in concrete_inputs:
                            tensor = concrete_inputs[inp_id]
                            logger.debug(f"  Found input {inp_id} in concrete_inputs: shape={tensor.shape if hasattr(tensor, 'shape') else 'N/A'}")
                            
                            # ✅ FIX: Same check for concrete inputs
                            if isinstance(tensor, LazyTensor):
                                raise AssertionError(
                                    f"BUG: LazyTensor in concrete_inputs\n"
                                    f"  This indicates factory operations are still intercepted.\n"
                                )
                            
                            resolved_inputs.append(tensor)
                        else:
                            logger.warning(f"  Input {inp_id} not found in result_cache or concrete_inputs for {entry.operation_name}")
                            logger.debug(f"  result_cache keys: {list(result_cache.keys())}")
                            logger.debug(f"  concrete_inputs keys: {list(concrete_inputs.keys())}")
                    
                    # Execute operation - use UniversalDispatcher for 99% of ops
                    # Check both base_name and full operation_name for handler lookup
                    base_name = entry.operation_name.replace("aten::", "")
                    op_handler = executor.operation_handlers.get(base_name) or executor.operation_handlers.get(entry.operation_name)
                    
                    if op_handler:
                        # Special case handlers (layer_norm, reshape, embedding, dropout, split, etc.)
                        logger.debug(f"Using handler {op_handler.__name__} for {entry.operation_name}")
                        dummy_lt = type('obj', (object,), {
                            'operation': entry.operation_name,
                            'kwargs': entry.kwargs  # Preserve original kwargs
                        })()
                        result = op_handler(dummy_lt, resolved_inputs, entry.kwargs)
                        logger.debug(f"Handler returned: type={type(result).__name__}, shape={result.shape if hasattr(result, 'shape') else 'N/A'}")
                    else:
                        # Default: use UniversalDispatcher (handles 99% automatically)
                        logger.debug(f"No handler found for {entry.operation_name}, using UniversalDispatcher")
                        result = executor._execute_fallback_eager(entry.operation_name, resolved_inputs, entry.kwargs)
                    
                    # ✅ FIX: Verify result is concrete
                    if isinstance(result, LazyTensor):
                        raise AssertionError(
                            f"BUG: Operation returned LazyTensor during materialization\n"
                            f"  Operation: {entry.operation_name}\n"
                            f"  Handler: {op_handler.__name__ if op_handler else 'fallback'}\n"
                            f"  This indicates interception is not properly disabled.\n"
                        )
                    
                    logger.debug(f"Storing result for {entry.operation_name}: output_id={entry.output_id}, shape={result.shape if hasattr(result, 'shape') else 'N/A'}")
                    result_cache[entry.output_id] = result
            
            # Synchronize streams - transfer_stream waits for compute_stream
            if self.transfer_stream is not None and self.compute_stream is not None:
                self.transfer_stream.wait_stream(self.compute_stream)
            torch.cuda.synchronize()

            # Return final result
            if schedule:
                final_output_id = schedule[-1].output_id
                final_result = result_cache.get(final_output_id, torch.tensor(0.0))
                
                # ✅ FIX: Final sanity check
                if isinstance(final_result, LazyTensor):
                    raise AssertionError(
                        f"BUG: Final result is LazyTensor\n"
                        f"  This should be impossible.\n"
                    )
                
                return final_result
            else:
                return torch.tensor(0.0)


class PinnedMemoryPool:
    """Reusable pinned memory for efficient CPU↔GPU transfers"""
    
    def __init__(self, pool_size_mb: int = 256):
        self.pool_size_bytes = pool_size_mb * 1024 * 1024
        self.pinned_buffers: Dict[int, torch.Tensor] = {}
        self.available_buffers: List[torch.Tensor] = []
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    def allocate(self, size_bytes: int) -> torch.Tensor:
        """Allocate pinned memory buffer"""
        # Find suitable buffer
        for buf in self.available_buffers:
            if buf.numel() * buf.element_size() >= size_bytes:
                self.available_buffers.remove(buf)
                return buf
        
        # Allocate new
        if torch.cuda.is_available():
            buf = torch.empty(size_bytes, dtype=torch.uint8, device='cpu',
                            pin_memory=True)
        else:
            buf = torch.empty(size_bytes, dtype=torch.uint8)
        
        return buf
    
    def release(self, buffer: torch.Tensor):
        """Release buffer back to pool"""
        if len(self.available_buffers) * buffer.numel() * buffer.element_size() < self.pool_size_bytes:
            self.available_buffers.append(buffer)


class BatchMaterializer:
    """Batch multiple materializations for efficiency"""
    
    def __init__(self, optimizer: MaterializationOptimizer):
        self.optimizer = optimizer
        self.pending_materializations: List[torch.Tensor] = []
    
    def add_pending(self, lazy_tensor):
        """Queue materialization"""
        self.pending_materializations.append(lazy_tensor)
    
    def materialize_batch(self, executor) -> List[torch.Tensor]:
        """Execute all pending materializations efficiently"""
        if not self.pending_materializations:
            return []
        
        results = []
        
        # Execute with minimal synchronization
        for lt in self.pending_materializations:
            result = self.optimizer.execute_optimized(lt, executor)
            results.append(result)
        
        self.pending_materializations.clear()
        return results


def optimize_materialization_for_executor(executor):
    """Wrap executor with optimized materialization"""
    original_execute = executor._execute_recursive
    optimizer = MaterializationOptimizer(enable_streams=True, enable_pinned_memory=True)
    
    def optimized_execute(target_lazy_tensor):
        """Use optimized materialization"""
        try:
            # Try optimized path first
            return optimizer.execute_optimized(target_lazy_tensor, executor)
        except Exception as e:
            logger.warning(f"Optimized materialization failed: {e}, falling back to original")
            # Fallback to original implementation
            return original_execute(target_lazy_tensor)
    
    executor._execute_recursive = optimized_execute
    executor.materialization_optimizer = optimizer
    
    return executor
