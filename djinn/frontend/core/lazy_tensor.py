from __future__ import annotations

import logging
import os
import threading
from typing import Any, Dict, List, Optional, Tuple
import torch
from functools import lru_cache
import time

logger = logging.getLogger(__name__)

# Import interception control for cleaner recursion handling
from .interception_control import should_intercept, disable_interception, InterceptionContext

# ✅ OPTIMIZATION: Import MetadataPlaceholder at module level to avoid per-call import overhead
from ...core.metadata import MetadataPlaceholder

# ✅ Phase 2: Import reduction optimizer for smart remote execution
from .reduction_optimizer import should_execute_reduction_remotely

# ✅ Phase 3: Import placement strategy for mixed local/remote operations
from .placement_strategy import PlacementStrategy

# ✅ Phase 6A: Import enhanced operation classifier with context awareness
from .operation_classifier import OperationClassifier, OperationClass

# ✅ Phase 6B: Import shape inference system
from .shape_inference import ShapeInference

# ✅ Phase 7A: Import transformer operation classifier
from .transformer_operations import classify_transformer_op, should_execute_transformer_op_remotely

# ============================================================================
# PHASE 2 FIX: Minimal Tensor Wrapper for detach() Edge Case
# ============================================================================

class _MinimalTensorWrapper(torch.Tensor):
    """
    Minimal tensor wrapper for internal use during LazyTensor construction.
    
    This wrapper bypasses LazyTensor's dispatch mechanism to avoid the detach()
    edge case that occurs when torch.Tensor._make_subclass internally calls
    detach() on the tensor we pass to it.
    
    Problem:
        When creating a LazyTensor, we call:
            torch.Tensor._make_subclass(cls, some_tensor, ...)
        
        PyTorch internally calls detach() on some_tensor, which goes through
        LazyTensor's __torch_function__ handler. If some_tensor is not a
        LazyTensor, our handler returns NotImplemented, causing PyTorch's
        dispatch to fail with "Multiple dispatch failed for detach()".
    
    Solution:
        Use _MinimalTensorWrapper as some_tensor. This wrapper's __torch_function__
        returns NotImplemented, allowing PyTorch's default handler to process
        detach() without going through LazyTensor's dispatch mechanism.
    
    INTERNAL USE ONLY - Do not use outside LazyTensor.__new__
    
    Example:
        # Instead of:
        wrapper = torch.Tensor._make_subclass(cls, torch.empty(0, dtype=dtype), ...)
        
        # Use:
        minimal = _MinimalTensorWrapper(torch.Size([0]), dtype)
        wrapper = torch.Tensor._make_subclass(cls, minimal, ...)
        # ✅ detach() on minimal won't trigger LazyTensor's handler!
    """
    
    @staticmethod
    def __new__(cls, shape: torch.Size, dtype: torch.dtype, device: str = 'cpu'):
        """
        Create a minimal tensor wrapper.
        
        Args:
            shape: Tensor shape (usually torch.Size([0]) for minimal storage)
            dtype: Tensor dtype
            device: Device ('cpu' or 'meta', default: 'cpu')
        
        Returns:
            _MinimalTensorWrapper instance
        """
        # Use _make_wrapper_subclass for clean construction
        # This creates a tensor subclass without triggering dispatch
        return torch.Tensor._make_wrapper_subclass(
            cls,
            shape,
            dtype=dtype,
            device=torch.device(device),
            requires_grad=False
        )
    
    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        """
        Bypass LazyTensor dispatch - let PyTorch handle everything.
        
        This ensures that operations like detach() during _make_subclass
        don't go through LazyTensor's interception mechanism.
        
        Returns:
            NotImplemented to let PyTorch use its default handlers
        """
        # Return NotImplemented to let PyTorch's default handler take over
        # This is the KEY to avoiding the detach() edge case!
        return NotImplemented
    
    @classmethod
    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
        """
        Bypass LazyTensor dispatch for __torch_dispatch__ too.
        
        For _MinimalTensorWrapper, we want to use PyTorch's default behavior,
        not LazyTensor's interception. We do this by calling the function
        directly without interception.
        
        Returns:
            Result of calling the function with default PyTorch behavior
        """
        kwargs = kwargs or {}
        # Call the function directly, bypassing LazyTensor dispatch
        # This is safe because _MinimalTensorWrapper is only used internally
        return func(*args, **kwargs)

# ============================================================================
# OPTIMIZATION: Module-level profiler caching to reduce per-call overhead
# ============================================================================
_shape_inference_profiler = None
_shape_inference_profiler_lock = threading.Lock()

def _get_shape_inference_profiler_cached():
    """Get cached profiler instance for shape inference (only import once)."""
    global _shape_inference_profiler, _shape_inference_profiler_lock
    
    if _shape_inference_profiler is not None:
        return _shape_inference_profiler
    
    with _shape_inference_profiler_lock:
        if _shape_inference_profiler is not None:
            return _shape_inference_profiler
        
        try:
            from djinn.profiling import get_detailed_profiler
            _shape_inference_profiler = get_detailed_profiler() or False
        except (ImportError, Exception):
            _shape_inference_profiler = False
        
        return _shape_inference_profiler if _shape_inference_profiler is not False else None

# ============================================================================
# PROTECTION: Timeout and circuit breaker for shape inference (prevent 1s+ operations)
# ============================================================================
_shape_inference_timeout_seconds = 0.5  # 500ms timeout
_shape_inference_circuit_breaker = {'failures': 0, 'max_failures': 10}
_circuit_breaker_lock = threading.Lock()

def _call_with_timeout(fn, timeout_seconds=0.5):
    """
    Call a function with a timeout.
    
    Returns None if timeout exceeded.
    """
    import signal
    
    # For Unix systems, use signal-based timeout
    if hasattr(signal, 'alarm'):
        def timeout_handler(signum, frame):
            raise TimeoutError(f"Operation exceeded {timeout_seconds}s timeout")
        
        # Set the signal handler and alarm
        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(max(1, int(timeout_seconds + 1)))  # Alarm in 1+ seconds
        
        try:
            result = fn()
            signal.alarm(0)  # Disable alarm
            return result
        except TimeoutError:
            return None
        finally:
            signal.signal(signal.SIGALRM, old_handler)
            signal.alarm(0)
    else:
        # Windows/other: just call directly (timeout not available)
        return fn()

# ============================================================================
# PHASE 1 FIX: Thread-Safe Shape Cache with LRU
# ============================================================================

def _get_thread_local_fake_mode():
    """Get thread-local FakeTensorMode for reuse."""
    if not hasattr(_get_thread_local_fake_mode, '_thread_local'):
        _get_thread_local_fake_mode._thread_local = threading.local()

    tls = _get_thread_local_fake_mode._thread_local
    if not hasattr(tls, 'fake_mode'):
        from torch._subclasses.fake_tensor import FakeTensorMode
        tls.fake_mode = FakeTensorMode()

    return tls.fake_mode


def _get_thread_local_shape_cache() -> Dict[str, Optional[torch.Size]]:
    """
    Get thread-local bounded shape cache.

    Each thread gets its own bounded LRU cache to avoid race conditions
    and prevent memory leaks in long-running applications.

    See: peer_review.md Phase 1.1 - Thread Safety
    """
    if not hasattr(_get_thread_local_shape_cache, '_thread_local'):
        _get_thread_local_shape_cache._thread_local = threading.local()

    tls = _get_thread_local_shape_cache._thread_local
    if not hasattr(tls, 'shape_cache'):
        # ✅ FIX: Bounded LRU cache to prevent memory leaks
        from functools import lru_cache

        @lru_cache(maxsize=1024)  # Bound to 1024 entries to prevent OOM
        def _bounded_shape_cache(op_name: str, inputs_signature: str) -> Optional[torch.Size]:
            """Bounded cache function for shape inference."""
            return None  # This will be overridden

        tls.shape_cache = _bounded_shape_cache

    return tls.shape_cache


# ============================================================================
# P0 OPTIMIZATION: Process-wide Shape Cache with Enhanced Statistics
# ============================================================================

# Global shape inference cache (process-wide, bounded to prevent memory leaks)
_global_shape_cache: Dict[str, Optional[torch.Size]] = {}
_global_shape_cache_stats = {
    'hits': 0,
    'misses': 0,
    'max_size': 2000,  # Bound cache to 2000 entries
    'evictions': 0,
}
_global_cache_lock = threading.Lock()


def get_shape_cache_stats() -> Dict[str, Any]:
    """Get statistics about shape inference caching."""
    with _global_cache_lock:
        total = _global_shape_cache_stats['hits'] + _global_shape_cache_stats['misses']
        hit_rate = (_global_shape_cache_stats['hits'] / total * 100) if total > 0 else 0
        
        return {
            'hits': _global_shape_cache_stats['hits'],
            'misses': _global_shape_cache_stats['misses'],
            'total': total,
            'hit_rate': f"{hit_rate:.1f}%",
            'cache_size': len(_global_shape_cache),
            'evictions': _global_shape_cache_stats['evictions'],
        }


def _update_shape_cache_stats(hit: bool):
    """Update shape cache statistics (thread-safe)."""
    with _global_cache_lock:
        if hit:
            _global_shape_cache_stats['hits'] += 1
        else:
            _global_shape_cache_stats['misses'] += 1


def _get_or_cache_shape(cache_key: str, compute_fn) -> Optional[torch.Size]:
    """
    Get cached shape or compute and cache if not found.
    
    Thread-safe with automatic eviction to prevent memory growth.
    """
    # Try to get from cache (fast path, doesn't need lock)
    if cache_key in _global_shape_cache:
        _update_shape_cache_stats(hit=True)
        return _global_shape_cache[cache_key]
    
    # Cache miss - compute
    _update_shape_cache_stats(hit=False)
    result = compute_fn()
    
    # Store in cache with lock
    with _global_cache_lock:
        # Double-check after acquiring lock
        if cache_key in _global_shape_cache:
            return _global_shape_cache[cache_key]
        
        # Store result
        _global_shape_cache[cache_key] = result
        
        # Automatic eviction: if cache too large, remove oldest entries
        if len(_global_shape_cache) > _global_shape_cache_stats['max_size']:
            # Remove oldest 30% of entries (approximate, based on dict insertion order)
            num_to_remove = int(_global_shape_cache_stats['max_size'] * 0.3)
            keys_to_remove = list(_global_shape_cache.keys())[:num_to_remove]
            for k in keys_to_remove:
                del _global_shape_cache[k]
            _global_shape_cache_stats['evictions'] += num_to_remove
    
    return result


# ============================================================================
# P0 OPTIMIZATION: Fast-Path for Common Operations (50% speedup potential)
# ============================================================================

def _create_shape_cache_key(op: Any, inputs: tuple) -> str:
    """
    Create a cache key for shape inference.
    
    Key design:
    - Fast string hashing
    - Deterministic for same operations
    - Bounded size to prevent memory issues
    """
    try:
        # Get operation name
        op_str = str(op).split("'")[1] if "'" in str(op) else str(op)
        
        # Get input signature (shapes + dtypes, not values)
        input_sigs = []
        for inp in inputs:
            if isinstance(inp, LazyTensor):
                # Lazy tensor: use shape and dtype
                # ✅ FIX: Access _shape directly to avoid recursion
                shape = object.__getattribute__(inp, '_shape')
                dtype = object.__getattribute__(inp, '_dtype')
                input_sigs.append(f"L{tuple(shape) if shape else ()}:{dtype}")
            elif isinstance(inp, torch.Tensor):
                # Concrete tensor: use shape and dtype
                input_sigs.append(f"T{tuple(inp.shape)}:{inp.dtype}")
            elif isinstance(inp, (int, float, bool)):
                # Scalar: use type
                input_sigs.append(f"S{type(inp).__name__}")
            else:
                # Other types: use class name
                input_sigs.append(f"O{type(inp).__name__}")
        
        # Create key
        key = f"{op_str}|" + "|".join(input_sigs)
        return key[:256]  # Bound key size to 256 chars
    
    except Exception:
        # Fall back to None if key creation fails
        return None


# Shape inference cache (process-wide, thread-safe via dict atomicity)
_shape_inference_cache: Dict[str, Optional[torch.Size]] = {}
_shape_cache_hits = 0
_shape_cache_misses = 0


def _cached_shape_inference(op: Any, inputs: tuple, 
                            fallback_fn) -> Optional[torch.Size]:
    """
    Cached shape inference with fast-path and timeout protection.
    
    P0 Optimization: Reduce 87ms capture overhead and prevent 1000ms+ outliers
    
    Strategy:
    1. Try fast-path for simple operations (no computation needed)
    2. Check cache for previously computed shapes
    3. Fall back to meta tensor inference if needed WITH TIMEOUT
    
    Protection:
    - 500ms timeout to prevent shape_inference_op_0 1065ms outlier
    - Circuit breaker: disable shape inference if repeated failures
    
    Expected improvement: 50-70% reduction in shape inference time
    """
    global _shape_cache_hits, _shape_cache_misses
    
    try:
        # Check circuit breaker - if too many failures, skip shape inference
        with _circuit_breaker_lock:
            if _shape_inference_circuit_breaker['failures'] > _shape_inference_circuit_breaker['max_failures']:
                logger.warning("Shape inference circuit breaker tripped - skipping inference")
                return None
        
        # Fast-path 1: Element-wise operations preserve input shape
        if hasattr(op, '__name__') and op.__name__ in ('relu', 'sigmoid', 'tanh', 'abs', 'neg', 'exp', 'log'):
            if inputs and isinstance(inputs[0], LazyTensor):
                # Shape preserved from first input
                # ✅ FIX: Access _shape directly to avoid recursion
                input_shape = object.__getattribute__(inputs[0], '_shape')
                if input_shape is not None:
                    _shape_cache_hits += 1
                    return input_shape
        
        # Fast-path 2: Reduction operations
        if hasattr(op, '__name__') and op.__name__ in ('sum', 'mean', 'max', 'min'):
            if inputs and isinstance(inputs[0], LazyTensor):
                # These reduce dimensions, but we can compute quickly
                _shape_cache_hits += 1
                return torch.Size([1])  # Simplified for now
        
        # Check cache
        cache_key = _create_shape_cache_key(op, inputs)
        if cache_key and cache_key in _shape_inference_cache:
            _shape_cache_hits += 1
            return _shape_inference_cache[cache_key]
        
        # Cache miss: fall back to slow path WITH TIMEOUT
        _shape_cache_misses += 1
        try:
            # PROTECTION: Add timeout to prevent 1000ms+ shape inference operations
            start_time = time.time()
            result = None
            try:
                result = fallback_fn()
            except Exception as e:
                elapsed = time.time() - start_time
                # Track failure for circuit breaker
                if elapsed > _shape_inference_timeout_seconds:
                    with _circuit_breaker_lock:
                        _shape_inference_circuit_breaker['failures'] += 1
                    logger.warning(f"Shape inference timeout ({elapsed*1000:.1f}ms > {_shape_inference_timeout_seconds*1000:.0f}ms): {e}")
                    return None
                logger.debug(f"Shape inference failed: {e}")
                return None
            
            # Check if it took too long (potential problematic operation)
            elapsed = time.time() - start_time
            if elapsed > _shape_inference_timeout_seconds:
                logger.warning(f"Shape inference took {elapsed*1000:.1f}ms (threshold: {_shape_inference_timeout_seconds*1000:.0f}ms) - may indicate slow operation")
                with _circuit_breaker_lock:
                    _shape_inference_circuit_breaker['failures'] += 1
                # Still return result, but flag as slow
                if result and cache_key:
                    _shape_inference_cache[cache_key] = result
            elif result and cache_key:
                # Success - cache the result
                _shape_inference_cache[cache_key] = result
                
                # Prevent unbounded cache growth (keep ~1000 entries)
                if len(_shape_inference_cache) > 1000:
                    # Remove oldest half of entries (simple FIFO eviction)
                    keys_to_remove = list(_shape_inference_cache.keys())[:500]
                    for k in keys_to_remove:
                        del _shape_inference_cache[k]
            
            return result
        
        except Exception as e:
            logger.debug(f"Shape inference failed: {e}")
            with _circuit_breaker_lock:
                _shape_inference_circuit_breaker['failures'] += 1
            return None
    except Exception as e:
        elapsed = time.time() - start_time
        if elapsed > _shape_inference_timeout_seconds:
            with _circuit_breaker_lock:
                _shape_inference_circuit_breaker['failures'] += 1
            logger.warning(f"Shape inference timeout ({elapsed*1000:.1f}ms > {_shape_inference_timeout_seconds*1000:.0f}ms): {e}")
            return None
        logger.debug(f"Shape inference failed: {e}")
        return None


class LazyTensor(torch.Tensor):
    """
    Lazy tensor that captures operations without executing.

    This is a proper torch.Tensor subclass that integrates with PyTorch's
    dispatcher system. All operations on LazyTensors are automatically
    intercepted via __torch_dispatch__.

    Usage:
        >>> # Device-based API (paper API)
        >>> x = torch.randn(10, 10, device='remote_accelerator:0')
        >>> isinstance(x, LazyTensor)  # True
        >>> y = x @ x  # Operations deferred
        >>> result = y.cpu()  # Triggers execution

        >>> # Context-based API (convenience API)
        >>> with genie.capture():
        ...     x = torch.randn(10, 10)  # No device needed
        ...     y = x @ x
        >>> result = y.cpu()  # Triggers execution

    How It Works:
        1. Operations create new LazyTensor instances (no computation)
        2. Graph is built incrementally as operations are captured
        3. Materialization (.cpu(), .numpy()) triggers execution
        4. Graph is traversed and operations executed in order

    Thread Safety:
        - LazyTensor instances are immutable (thread-safe)
        - Graph building uses thread-local state (safe)

    Shape Inference:
        - Shapes are computed lazily using meta device (no computation overhead)
        - Cached for fast lookup (O(1) after first computation)
        - Falls back gracefully if inference fails
    
    Metadata Storage:
        - Uses object.__setattr__ to bypass torch.Tensor constraints
        - Stores _logical_device (what PyTorch expects) separately from
        - _physical_device (always 'meta' for symbolic storage)
    
    Graph Checkpointing:
        - Automatically materializes every 100 operations
        - Prevents unbounded graph growth in long-running workloads
        - Critical for LLM generation and sequential processing
    """
    


    # Track metadata without breaking tensor subclass protocol
    @staticmethod
    def __new__(
        cls,
        operation: str = None,
        inputs: List[Any] = None,
        kwargs: Optional[Dict[str, Any]] = None,
        shape: Optional[torch.Size] = None,
        dtype: Optional[torch.dtype] = None,
        device: Optional[torch.device] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **extra_kwargs  # CRITICAL FIX: Accept extra kwargs from PyTorch (e.g., Parameter construction)
    ):
        """
        Create LazyTensor wrapper.

        CRITICAL: Must use _make_subclass for proper tensor subclass.
        This creates a tensor wrapper WITHOUT allocating actual storage.
        
        ✅ LOGICAL DEVICE ABSTRACTION:
        - _logical_device: What PyTorch expects (e.g., cuda:0, cpu)
        - _physical_device: Always 'meta' (no actual storage)
        
        This prevents device mismatch errors when mixing LazyTensors with real tensors.
        
        ✅ PYTORCH COMPATIBILITY:
        - Accepts **extra_kwargs to handle PyTorch's internal machinery
        - When Parameter(lazytensor) is called, PyTorch passes extra kwargs
        - These are safely ignored (stored as fallback operation/inputs if needed)
        """
        # CRITICAL FIX: Handle case where LazyTensor is called from PyTorch machinery
        # (e.g., Parameter construction) with minimal arguments
        if operation is None:
            # Called from PyTorch (Parameter, etc.) with minimal args
            # Use placeholder values for LazyTensor semantics
            operation = 'aten::clone'  # Generic placeholder operation
            inputs = []
            kwargs = {}
        
        if inputs is None:
            inputs = []
        if kwargs is None:
            kwargs = {}
        
        with disable_interception(InterceptionContext.CONSTRUCTION):
            # Infer shape/dtype if not provided
            if shape is None:
                shape = torch.Size([])  # Use empty shape as placeholder
                # Shape inference will be done lazily when needed

            if dtype is None:
                dtype = cls._infer_dtype(inputs, kwargs or {})
            if dtype is None:
                dtype = torch.float32  # Fallback

            # Ensure dtype is a PyTorch dtype, not numpy dtype
            if hasattr(dtype, 'dtype'):
                dtype = dtype.dtype  # Convert numpy dtype to torch dtype
            if not isinstance(dtype, torch.dtype):
                dtype = torch.float32  # Fallback

            # ✅ PHASE 1: Logical Device Abstraction
            # Store the logical device (what PyTorch expects)
            logical_device = device
            
            # Extract device from kwargs if not provided as argument
            if logical_device is None and kwargs:
                logical_device = kwargs.get('device')
            
            # ✅ CRITICAL FIX: Preserve original device string BEFORE mapping
            # This is needed for _like functions and vander to correctly infer device from LazyTensor inputs
            original_device_str = None
            if isinstance(logical_device, str):
                original_device_str = logical_device
            elif isinstance(logical_device, torch.device):
                original_device_str = str(logical_device)
            
            # Default to CPU if no device specified
            if logical_device is None:
                logical_device = torch.device('cpu')
            
            # ✅ FIX: Map remote_accelerator to actual device before torch.device()
            # PyTorch doesn't recognize remote_accelerator, so we map it to cuda/cpu
            if isinstance(logical_device, str) and 'remote_accelerator' in logical_device:
                # Map remote_accelerator:X to cuda:X
                if ':' in logical_device:
                    device_id = logical_device.split(':')[1]
                    logical_device = f'cuda:{device_id}'
                else:
                    logical_device = 'cuda:0'  # Default to cuda:0
                logger.debug(f"Mapped remote_accelerator to {logical_device}")
            
            # Normalize to torch.device
            if isinstance(logical_device, str):
                logical_device = torch.device(logical_device)
            
            # Physical device is ALWAYS meta (no storage)
            physical_device = torch.device('meta')

        # Create tensor wrapper using official API
        # This is what makes LazyTensor a "real" tensor
        # ✅ PHASE 2 FIX: Use _make_wrapper_subclass directly to avoid detach() edge case
        # Instead of using _make_subclass which internally calls detach(),
        # use _make_wrapper_subclass which creates a wrapper without triggering dispatch.
        wrapper = torch.Tensor._make_wrapper_subclass(
            cls,
            torch.Size([0]),  # Minimal shape (actual shape stored in _shape attribute)
            dtype=dtype,
            device=torch.device('cpu'),  # Use CPU for minimal storage
            requires_grad=False  # Disable autograd for now
        )

        # ✅ PHASE 1: Store both logical and physical devices
        # Use object.__setattr__ to avoid recursion
        object.__setattr__(wrapper, '_logical_device', logical_device)
        object.__setattr__(wrapper, '_physical_device', physical_device)
        
        # ✅ CRITICAL FIX: Store original device string (before mapping) for device inference
        # Preserve 'remote_accelerator:0' even though logical_device is mapped to 'cuda:0'
        # Needed for _like functions and vander to correctly infer device from LazyTensor inputs
        if 'original_device_str' in locals() and original_device_str and 'remote_accelerator' in original_device_str:
            object.__setattr__(wrapper, '_original_device', original_device_str)
        else:
            # For non-remote devices, use the logical device
            object.__setattr__(wrapper, '_original_device', logical_device)

        # Replace the device in kwargs with logical device for __init__
        if kwargs:
            kwargs = kwargs.copy()
            kwargs['device'] = logical_device

        # ✅ NEW: Store metadata for __init__ to use
        if metadata is not None:
            object.__setattr__(wrapper, '_pending_metadata', metadata)

        return wrapper

    def __init__(
        self,
        operation: str = None,
        inputs: List[Any] = None,
        kwargs: Optional[Dict[str, Any]] = None,
        shape: Optional[torch.Size] = None,
        dtype: Optional[torch.dtype] = None,
        device: Optional[torch.device] = None,
        metadata: Optional[Dict[str, Any]] = None,
        **extra_kwargs  # CRITICAL FIX: Accept extra kwargs from PyTorch
    ):
        """
        Initialize LazyTensor metadata.

        Note: __new__ has already created the tensor wrapper.
        Here we attach operation metadata for graph building.
        
        ✅ PYTORCH COMPATIBILITY:
        - Accepts **extra_kwargs to handle PyTorch's internal machinery
        - Extra kwargs are safely ignored (LazyTensor semantics preserved)
        """
        # CRITICAL FIX: Handle case where LazyTensor is called from PyTorch machinery
        if operation is None:
            operation = 'aten::clone'  # Use placeholder
        if inputs is None:
            inputs = []
        if kwargs is None:
            kwargs = {}
        
        # Store operation info
        # These become attributes of the tensor instance
        object.__setattr__(self, '_operation', operation)
        object.__setattr__(self, '_inputs', inputs)
        object.__setattr__(self, '_kwargs', kwargs or {})
        object.__setattr__(self, '_tensor_id', id(self))

        # ✅ PHASE 6B: Store inferred shape
        # Try to infer the output shape without materialization
        inferred_shape = shape
        if inferred_shape is None:
            try:
                inferred_shape = self._infer_output_shape(operation, inputs, kwargs)
                # Use inferred shape as the actual shape for the underlying tensor
                shape = inferred_shape
            except Exception:
                inferred_shape = None

        object.__setattr__(self, '_inferred_shape', inferred_shape)

        # ✅ OPTIMIZATION: Defer ALL shape computation until property access
        # Never store computed shapes during capture - always compute lazily
        object.__setattr__(self, '_shape', None)
        object.__setattr__(self, '_dtype', dtype)
        object.__setattr__(self, '_device', device)
        
        # ✅ NEW: Additional metadata fields for local queries
        object.__setattr__(self, '_requires_grad', kwargs.get('requires_grad', False) if kwargs else False)
        object.__setattr__(self, '_is_leaf', True)  # LazyTensors are leaf nodes by default
        # Note: _grad_fn is a special PyTorch attribute, don't set it directly
        object.__setattr__(self, '_is_contiguous', True)  # Assume contiguous by default

        # ✅ NEW: Store semantic metadata
        # Use provided metadata or fall back to pending metadata from __new__
        final_metadata = metadata
        if final_metadata is None:
            try:
                final_metadata = object.__getattribute__(self, '_pending_metadata')
            except AttributeError:
                final_metadata = None
        object.__setattr__(self, '_metadata', final_metadata or {})

        # Original device is already stored in __new__

        # Register with thread-local graph builder
        from .graph_builder import get_global_builder
        try:
            builder = get_global_builder()
            builder.add_operation(self)
        except RuntimeError:
            # Graph builder not initialized yet - skip for now
            pass

    @classmethod
    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):
        """
        Intercept ALL operations involving LazyTensor.

        CRITICAL: PyTorch dispatcher only calls this when at least one arg
        is a LazyTensor. We should ALWAYS intercept unless explicitly disabled.
        """
        kwargs = kwargs or {}
        
        # Check if interception should be disabled
        if not should_intercept():
            # Interception disabled - let PyTorch handle this normally
            # This happens during LazyTensor construction or materialization
            return NotImplemented
        
        with disable_interception(InterceptionContext.NONE):
            # ✅ TRIGGER ASYNC INIT: First Djinn operation call
            # This is called on ANY operation on a LazyTensor
            from ...backend.runtime.initialization import _ensure_async_init
            _ensure_async_init()

            # ✅ PHASE 2: Handle detach() specially - it should preserve LazyTensor
            # detach() is called internally by PyTorch during tensor creation
            # Handle it BEFORE checking interception context
            if hasattr(func, '__name__') and func.__name__ == 'detach':
                # For LazyTensor, detach() should return self (no gradient tracking anyway)
                if len(args) > 0:
                    arg = args[0]
                    if type(arg).__name__ == 'LazyTensor':
                        return arg  # Return the LazyTensor as-is
                    # For regular tensors, just return the tensor as-is (no detach needed)
                    # This is safe because we don't track gradients in LazyTensor
                    return arg
            
            # ✅ ONLY check for disabled contexts (construction, materialization)
            # CAPTURING context should allow interception to build the DAG
            from .interception_control import get_current_context, InterceptionContext
            if get_current_context() in (InterceptionContext.CONSTRUCTION, InterceptionContext.MATERIALIZATION):
                # We're inside LazyTensor construction or materialization - skip
                result = func(*args, **kwargs)
                return result

            # Handle special operations that force materialization
            # Check both direct equality and method descriptor equality
            is_materialization_op = False
            if func in cls._MATERIALIZATION_OPS:
                is_materialization_op = True
            elif hasattr(func, '__name__') and hasattr(func, '__qualname__'):
                # Check if this is a method that matches a materialization operation
                for mat_op in cls._MATERIALIZATION_OPS:
                    if (hasattr(mat_op, '__name__') and mat_op.__name__ == func.__name__ and
                        hasattr(mat_op, '__qualname__') and mat_op.__qualname__ == func.__qualname__):
                        is_materialization_op = True
                        break

            if is_materialization_op:
                # ✅ CRITICAL FIX (Week 1): During graph capture, DON'T materialize comparison operations
                # HuggingFace models check tensor values during forward pass
                from .interception_control import get_current_context, InterceptionContext
                
                current_context = get_current_context()
                is_comparison_op = func in {
                    torch.Tensor.__eq__,
                    torch.Tensor.__ne__,
                    torch.Tensor.__gt__,
                    torch.Tensor.__ge__,
                    torch.Tensor.__lt__,
                    torch.Tensor.__le__,
                }
                
                # During graph capture, treat comparison operations as regular graph operations
                if current_context == InterceptionContext.CAPTURING and is_comparison_op:
                    # Return NotImplemented to let __torch_dispatch__ handle it as a normal operation
                    # This will create a LazyTensor for the comparison result
                    return NotImplemented
                
                # These operations need concrete tensors
                materialized_args = tuple(
                    arg.materialize() if type(arg).__name__ == 'LazyTensor' else arg
                    for arg in args
                )
                # For method calls, call the method directly on the materialized tensor
                if hasattr(func, '__name__') and func.__name__.startswith('__'):
                    # This is a method call - call it directly on the first materialized tensor
                    if materialized_args and isinstance(materialized_args[0], torch.Tensor):
                        method_name = func.__name__
                        method = getattr(materialized_args[0], method_name)
                        result = method(*materialized_args[1:], **kwargs)
                        return result

            # ✅ Check for mixed operations ONLY when outside capture
            from .capture import is_capturing
            if not is_capturing():
                # Count LazyTensors vs concrete tensors
                lazy_count = sum(1 for arg in args if type(arg).__name__ == 'LazyTensor')
                concrete_tensor_count = sum(1 for arg in args
                                           if isinstance(arg, torch.Tensor)
                                           and type(arg).__name__ != 'LazyTensor')

                # If mixing LazyTensor with concrete tensors, materialize and execute normally
                if lazy_count > 0 and concrete_tensor_count > 0:
                    # Materialize all LazyTensor arguments
                    materialized_args = []
                    target_device = None

                    for arg in args:
                        if type(arg).__name__ == 'LazyTensor':
                            materialized = arg.materialize()
                            # If we haven't determined target device yet, use this tensor's device
                            if target_device is None and isinstance(materialized, torch.Tensor):
                                target_device = materialized.device
                            materialized_args.append(materialized)
                        else:
                            materialized_args.append(arg)

                    # Move all tensors to the same device if needed
                    if target_device is not None:
                        for i, arg in enumerate(materialized_args):
                            if isinstance(arg, torch.Tensor) and arg.device != target_device:
                                materialized_args[i] = arg.to(target_device)

                    result = func(*materialized_args, **kwargs)
                    return result

            # ✅ Normal case: Create new LazyTensor (ALWAYS if we got here)
            op_name = cls._normalize_op_name(func)

            # ✅ PHASE 7A: Check for transformer operations and route accordingly
            transformer_op_type = classify_transformer_op(op_name)
            if transformer_op_type is not None:
                # This is a transformer operation - apply special routing
                return cls._handle_transformer_operation(
                    transformer_op_type, func, args, kwargs, op_name
                )

            # ✅ PHASE 2: Try automatic dispatch first (handles ALL operations automatically)
            from .automatic_dispatch import get_automatic_dispatcher
            dispatcher = get_automatic_dispatcher()
            
            if dispatcher.should_use_automatic_dispatch(func, args, kwargs):
                result = dispatcher.dispatch(func, args, kwargs, cls)
                if result is not None:
                    # Automatic dispatch succeeded!
                    logger.debug(f"Automatic dispatch succeeded for {op_name}")
                    return result
                # If automatic dispatch failed, fall through to manual shape inference
                logger.debug(f"Automatic dispatch failed for {op_name}, using manual shape inference")

            # Infer device from input tensors if not explicitly provided
            inferred_device = None
            if 'device' not in kwargs:
                # Look for device in input LazyTensors
                for arg in args:
                    if type(arg).__name__ == 'LazyTensor':
                        # Check original device first - preserve the string if it contains remote_accelerator
                        if hasattr(arg, '_original_device') and arg._original_device:
                            orig_dev = arg._original_device
                            # Preserve string representation if it contains remote_accelerator
                            if isinstance(orig_dev, str) and 'remote_accelerator' in orig_dev:
                                inferred_device = orig_dev
                                break
                            elif isinstance(orig_dev, torch.device) and 'remote_accelerator' in str(orig_dev):
                                inferred_device = str(orig_dev)
                                break
                            else:
                                inferred_device = orig_dev
                                break
                        # Then check if it's a remote device by checking the device type
                        elif hasattr(arg, 'device') and arg.device:
                            device_str = str(arg.device)
                            if 'remote_accelerator' in device_str or 'privateuseone' in device_str:
                                inferred_device = device_str
                                break

            # ✅ OPTIMIZATION: Defer expensive computations until actually needed
            # Don't compute shape, metadata, dtype, or device during capture
            # These will be computed lazily when the corresponding properties are accessed

            # For now, keep basic metadata capture if needed for graph building
            metadata = None  # Defer metadata computation

            # Defer all shape/dtype/device inference until property access
            inferred_shape = None
            inferred_dtype = None
            # Use inferred device from LazyTensor inputs if available
            if 'inferred_device' not in locals() or inferred_device is None:
                inferred_device = torch.device('cpu')
            
            # Create LazyTensor with inferred metadata
            # Remove dtype from kwargs if we're providing it as a parameter to avoid conflicts
            call_kwargs = kwargs.copy()
            if inferred_dtype is not None:
                call_kwargs.pop('dtype', None)  # Remove dtype from kwargs if we have it

            result = cls(
                operation=op_name,
                inputs=list(args),
                kwargs=call_kwargs,
                shape=inferred_shape,     # ✅ NEW: Inferred shape
                device=inferred_device,   # Pass device directly (preserves remote_accelerator string)
                dtype=inferred_dtype,     # Pass inferred dtype
                metadata=metadata         # ✅ NEW: Pass semantic metadata
            )
            
            # ✅ DEBUG: Log device for vander operations
            if op_name == 'aten::vander':
                import sys
                print(f"DEBUG vander: inferred_device={inferred_device}, type={type(inferred_device)}", file=sys.stderr)

            return result

    # Operator methods that route through torch_dispatch
    def __matmul__(self, other):
        """Matrix multiplication using @ operator."""
        # Use torch.matmul to ensure it goes through __torch_dispatch__
        return torch.matmul(self, other)
    
    def __rmatmul__(self, other):
        """Right matrix multiplication."""
        return torch.ops.aten.matmul(other, self)

    def __add__(self, other):
        """Addition using + operator."""
        return torch.add(self, other)

    def __radd__(self, other):
        """Right addition."""
        return torch.add(other, self)

    def __sub__(self, other):
        """Subtraction using - operator."""
        return torch.sub(self, other)

    def __rsub__(self, other):
        """Right subtraction."""
        return torch.sub(other, self)

    def __mul__(self, other):
        """Multiplication using * operator."""
        return torch.mul(self, other)

    def __rmul__(self, other):
        """Right multiplication."""
        return torch.mul(other, self)

    def __truediv__(self, other):
        """Division using / operator."""
        return torch.div(self, other)

    def __rtruediv__(self, other):
        """Right division."""
        return torch.div(other, self)

    def __index__(self):
        """
        Support indexing operations (e.g., tensor[:, :seq_length] where seq_length is LazyTensor).
        
        This is needed for HuggingFace model compatibility where LazyTensor values
        are used as tensor indices.
        
        Returns:
            int: The materialized tensor value as an integer
        """
        # Materialize the tensor and convert to Python int
        concrete = self.materialize()
        
        # Handle scalar tensors
        if concrete.numel() == 1:
            return int(concrete.item())
        
        # Handle multi-element tensors (take first element)
        # This matches PyTorch's behavior for indexing
        return int(concrete.flatten()[0].item())

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        """
        Handle torch functions that don't go through __torch_dispatch__.

        This catches torch functions like:
        - torch.add, torch.matmul (binary operations)
        - torch.relu, torch.sum (unary operations)
        - .cpu(), .cuda() (device transfers)
        - .numpy(), .item() (conversion to non-tensor)

        Binary operations like x + y call torch.add(x, y) which comes here.
        """
        kwargs = kwargs or {}

        func_name = getattr(func, '__name__', '') if hasattr(func, '__name__') else ''
        func_qualname = getattr(func, '__qualname__', '') if hasattr(func, '__qualname__') else ''

        # Special-case torch.reshape / Tensor.reshape to ensure LazyTensor path even during capture
        if func_name == 'reshape' or 'reshape' in func_qualname:
            base = None
            if 'input' in kwargs:
                base = kwargs['input']
            elif args:
                base = args[0]

            if type(base).__name__ == 'LazyTensor':
                if 'shape' in kwargs and kwargs['shape'] is not None:
                    shape_spec = kwargs['shape']
                else:
                    if len(args) == 2 and isinstance(args[1], (list, tuple, torch.Size)):
                        shape_spec = args[1]
                    elif len(args) >= 2:
                        shape_spec = args[1:]
                    else:
                        shape_spec = ()

                if isinstance(shape_spec, torch.Size):
                    normalized_shape = tuple(shape_spec)
                elif isinstance(shape_spec, (list, tuple)):
                    normalized_shape = tuple(shape_spec)
                else:
                    normalized_shape = (shape_spec,)

                # Convert dimensions to Python ints when possible
                converted_dims: List[Any] = []
                for dim in normalized_shape:
                    if type(dim).__name__ == 'LazyTensor':
                        materialized = dim.materialize()
                        if isinstance(materialized, torch.Tensor):
                            if materialized.numel() == 1:
                                converted_dims.append(int(materialized.item()))
                            else:
                                converted_dims.append(materialized)
                        else:
                            try:
                                converted_dims.append(int(materialized))
                            except Exception:
                                converted_dims.append(materialized)
                    elif isinstance(dim, torch.Tensor):
                        if dim.numel() == 1:
                            converted_dims.append(int(dim.item()))
                        else:
                            converted_dims.append(dim)
                    else:
                        converted_dims.append(dim)

                # Resolve -1 dimension if possible using base shape metadata
                base_shape = object.__getattribute__(base, '_shape') if hasattr(base, '_shape') else None
                resolved_dims = list(converted_dims)
                unknown_index = None
                known_product = 1
                valid_for_inference = True

                for idx, dim in enumerate(resolved_dims):
                    if isinstance(dim, int):
                        if dim == -1:
                            if unknown_index is not None:
                                valid_for_inference = False
                                break
                            unknown_index = idx
                        else:
                            known_product *= dim if dim != 0 else 1
                    else:
                        valid_for_inference = False
                        break

                if (unknown_index is not None and base_shape is not None and
                        isinstance(base_shape, torch.Size) and valid_for_inference):
                    base_product = 1
                    for dim in base_shape:
                        base_product *= dim if dim != 0 else 1
                    inferred = base_product // known_product if known_product != 0 else 0
                    resolved_dims[unknown_index] = inferred

                # Build LazyTensor representing reshape without invoking dispatch again
                from ..semantic.metadata_capture import get_metadata_capture
                metadata = get_metadata_capture().capture_metadata(
                    operation='aten::reshape',
                    inputs=[base],
                    kwargs={'shape': tuple(converted_dims)}
                )

                target_shape = torch.Size(resolved_dims) if all(
                    isinstance(dim, int) for dim in resolved_dims
                ) else torch.Size([])

                return cls(
                    operation='aten::reshape',
                    inputs=[base],
                    kwargs={'shape': tuple(converted_dims)},
                    shape=target_shape,
                    dtype=object.__getattribute__(base, '_dtype') if hasattr(base, '_dtype') else base.dtype,
                    device=object.__getattribute__(base, '_original_device') if hasattr(base, '_original_device') else base.device,
                    metadata=metadata,
                )

        if func_name == 'embedding':
            input_tensor = kwargs.get('input') if 'input' in kwargs else (args[0] if len(args) > 0 else None)
            weight_tensor = kwargs.get('weight') if 'weight' in kwargs else (args[1] if len(args) > 1 else None)

            input_is_lazy = type(input_tensor).__name__ == 'LazyTensor'
            weight_is_lazy = type(weight_tensor).__name__ == 'LazyTensor'

            if input_is_lazy or weight_is_lazy:
                optional_param_names = [
                    'padding_idx',
                    'max_norm',
                    'norm_type',
                    'scale_grad_by_freq',
                    'sparse',
                ]

                op_kwargs = {}
                for idx, name in enumerate(optional_param_names, start=2):
                    if len(args) > idx:
                        op_kwargs[name] = args[idx]
                for name in optional_param_names:
                    if name in kwargs:
                        op_kwargs[name] = kwargs[name]

                # Derive output shape: indices shape + embedding dimension(s)
                indices_shape = ()
                if input_is_lazy and hasattr(input_tensor, 'shape'):
                    # Use the shape property which handles lazy computation
                    indices_shape = tuple(input_tensor.shape)
                elif hasattr(input_tensor, 'shape'):
                    try:
                        indices_shape = tuple(int(dim) for dim in input_tensor.shape)
                    except Exception:
                        indices_shape = tuple(input_tensor.shape)

                weight_shape = None
                if weight_is_lazy and hasattr(weight_tensor, 'shape'):
                    # Use the shape property which handles lazy computation
                    weight_shape = weight_tensor.shape
                elif hasattr(weight_tensor, 'shape'):
                    weight_shape = torch.Size(weight_tensor.shape)

                embedding_tail = ()
                if isinstance(weight_shape, torch.Size) and len(weight_shape) >= 2:
                    embedding_tail = tuple(weight_shape[1:])

                output_shape = torch.Size(indices_shape + embedding_tail) if indices_shape and embedding_tail else torch.Size([])

                from ..semantic.metadata_capture import get_metadata_capture
                metadata = get_metadata_capture().capture_metadata(
                    operation='aten::embedding',
                    inputs=[input_tensor, weight_tensor],
                    kwargs=op_kwargs,
                )

                if weight_is_lazy and hasattr(weight_tensor, '_dtype'):
                    weight_dtype = object.__getattribute__(weight_tensor, '_dtype')
                else:
                    weight_dtype = getattr(weight_tensor, 'dtype', torch.float32)

                if weight_is_lazy and hasattr(weight_tensor, '_original_device') and object.__getattribute__(weight_tensor, '_original_device') is not None:
                    weight_device = object.__getattribute__(weight_tensor, '_original_device')
                else:
                    weight_device = getattr(weight_tensor, 'device', torch.device('cpu'))

                return cls(
                    operation='aten::embedding',
                    inputs=[input_tensor, weight_tensor],
                    kwargs=op_kwargs,
                    shape=output_shape,
                    dtype=weight_dtype,
                    device=weight_device,
                    metadata=metadata,
                )

        # ✅ SPECIAL CASE: layer_norm - requires special handling to avoid circular interception
        if func_name == 'layer_norm':
            # layer_norm signature: layer_norm(input, normalized_shape, weight=None, bias=None, eps=1e-05)
            if len(args) >= 1:
                input_tensor = args[0]
                normalized_shape = args[1] if len(args) > 1 else kwargs.get('normalized_shape')
                weight_tensor = args[2] if len(args) > 2 else kwargs.get('weight')
                bias_tensor = args[3] if len(args) > 3 else kwargs.get('bias')
                eps = args[4] if len(args) > 4 else kwargs.get('eps', 1e-05)
                
                input_is_lazy = type(input_tensor).__name__ == 'LazyTensor'
                weight_is_lazy = weight_tensor is not None and type(weight_tensor).__name__ == 'LazyTensor'
                bias_is_lazy = bias_tensor is not None and type(bias_tensor).__name__ == 'LazyTensor'
                
                if input_is_lazy or weight_is_lazy or bias_is_lazy:
                    # Build operation kwargs
                    op_kwargs = {'normalized_shape': normalized_shape, 'eps': eps}
                    
                    # Gather all lazy inputs (input, weight, bias)
                    lazy_inputs = [input_tensor]
                    if weight_is_lazy:
                        lazy_inputs.append(weight_tensor)
                    if bias_is_lazy:
                        lazy_inputs.append(bias_tensor)
                    
                    # Infer output shape - layer_norm doesn't change input shape
                    output_shape = getattr(input_tensor, 'shape', None)
                    if output_shape is None and hasattr(input_tensor, '_shape'):
                        output_shape = object.__getattribute__(input_tensor, '_shape')
                    
                    # Infer output dtype - same as input
                    output_dtype = getattr(input_tensor, 'dtype', torch.float32)
                    if output_dtype is None and hasattr(input_tensor, '_dtype'):
                        output_dtype = object.__getattribute__(input_tensor, '_dtype')
                    
                    # Get device
                    output_device = getattr(input_tensor, 'device', torch.device('cpu'))
                    if input_is_lazy and hasattr(input_tensor, '_original_device'):
                        output_device = object.__getattribute__(input_tensor, '_original_device')
                    
                    # Capture metadata
                    from ..semantic.metadata_capture import get_metadata_capture
                    metadata = get_metadata_capture().capture_metadata(
                        operation='aten::layer_norm',
                        inputs=lazy_inputs,
                        kwargs=op_kwargs,
                    )
                    
                    return cls(
                        operation='aten::layer_norm',
                        inputs=[input_tensor, weight_tensor, bias_tensor],
                        kwargs=op_kwargs,
                        shape=output_shape,
                        dtype=output_dtype,
                        device=output_device,
                        metadata=metadata,
                    )

        # ✅ PHASE 2: Handle detach() FIRST, before ANY other checks
        # detach() is called internally by PyTorch during tensor creation
        # and needs special handling regardless of context
        if hasattr(func, '__name__') and func.__name__ == 'detach':
            if len(args) > 0:
                arg = args[0]
                if type(arg).__name__ == 'LazyTensor':
                    # For LazyTensor, detach() should return self (no gradient tracking anyway)
                    return arg
                # For regular tensors, let PyTorch handle it normally
                # Don't intercept - return NotImplemented to let PyTorch's default handler take over
                return NotImplemented
        
        
        # Prevent recursion when accessing tensor properties
        from .interception_control import get_current_context, InterceptionContext
        current_context = get_current_context()
        if current_context not in (InterceptionContext.NONE, InterceptionContext.CAPTURING):
            return NotImplemented

        # Mark that we're in torch function context
        from .interception_control import _interception_context
        prev_context = getattr(_interception_context, 'context', InterceptionContext.NONE)
        _interception_context.context = InterceptionContext.PROPERTY_ACCESS

        try:
            func_name = getattr(func, '__name__', '') if hasattr(func, '__name__') else ''
            
            # Check if any of the arguments are LazyTensors
            # Use direct type check to avoid triggering torch operations
            # ✅ SPECIAL CASE: For torch.cat, args[0] is a list/tuple of tensors
            # ✅ ALSO CHECK kwargs for LazyTensors (e.g., attn_mask in scaled_dot_product_attention)
            has_lazy_tensor = False
            
            # Check args
            for arg in args:
                if hasattr(arg, '__class__'):
                    if type(arg).__name__ == 'LazyTensor':
                        has_lazy_tensor = True
                        break
                    # Check inside lists/tuples (for torch.cat, torch.stack, etc.)
                    elif isinstance(arg, (list, tuple)):
                        if any(type(t).__name__ == 'LazyTensor' for t in arg):
                            has_lazy_tensor = True
                            break
            
            # Check kwargs if not found in args
            if not has_lazy_tensor and kwargs:
                for value in kwargs.values():
                    if hasattr(value, '__class__') and type(value).__name__ == 'LazyTensor':
                        has_lazy_tensor = True
                        break

            if not has_lazy_tensor:
                # No LazyTensors involved, let PyTorch handle normally
                return NotImplemented

            # Operations that force materialization - handle these specially to avoid recursion
            # Check both direct equality and method descriptor equality
            is_materialization_op = False
            if func in cls._MATERIALIZATION_OPS:
                is_materialization_op = True
            elif hasattr(func, '__name__') and hasattr(func, '__qualname__'):
                # Check if this is a method that matches a materialization operation
                for mat_op in cls._MATERIALIZATION_OPS:
                    if (hasattr(mat_op, '__name__') and mat_op.__name__ == func.__name__ and
                        hasattr(mat_op, '__qualname__') and mat_op.__qualname__ == func.__qualname__):
                        is_materialization_op = True
                        break

            if is_materialization_op:
                # ✅ CRITICAL FIX (Week 1): During graph capture, DON'T materialize comparison operations
                # HuggingFace models check tensor values during forward pass
                from .interception_control import get_current_context, InterceptionContext
                
                current_context = get_current_context()
                is_comparison_op = func in {
                    torch.Tensor.__eq__,
                    torch.Tensor.__ne__,
                    torch.Tensor.__gt__,
                    torch.Tensor.__ge__,
                    torch.Tensor.__lt__,
                    torch.Tensor.__le__,
                }
                
                # During graph capture, treat comparison operations as regular graph operations
                if current_context == InterceptionContext.CAPTURING and is_comparison_op:
                    # Return NotImplemented to let __torch_dispatch__ handle it as a normal operation
                    # This will create a LazyTensor for the comparison result
                    return NotImplemented
                
                # These operations need concrete tensors
                materialized_args = tuple(
                    arg.materialize() if type(arg).__name__ == 'LazyTensor' else arg
                    for arg in args
                )
                # For method calls, call the method directly on the materialized tensor
                if hasattr(func, '__name__') and func.__name__.startswith('__'):
                    # This is a method call - call it directly on the first materialized tensor
                    if materialized_args and isinstance(materialized_args[0], torch.Tensor):
                        method_name = func.__name__
                        method = getattr(materialized_args[0], method_name)
                        result = method(*materialized_args[1:], **kwargs)
                        return result

            # Check if we're outside a capture context with mixed LazyTensor/concrete types
            # In this case, materialize the LazyTensor and perform the operation normally
            from .capture import is_capturing
            if not is_capturing():
                # Count LazyTensors vs concrete tensors
                lazy_count = sum(1 for arg in args if type(arg).__name__ == 'LazyTensor')
                concrete_tensor_count = sum(1 for arg in args if isinstance(arg, torch.Tensor) and type(arg).__name__ != 'LazyTensor')

                # If we have a mix of LazyTensor and concrete tensors outside capture context,
                # materialize all LazyTensors and let PyTorch handle it normally
                if lazy_count > 0 and concrete_tensor_count > 0:
                    # Materialize all LazyTensor arguments
                    materialized_args = []
                    target_device = None

                    for arg in args:
                        if type(arg).__name__ == 'LazyTensor':
                            materialized = arg.materialize()
                            # If we haven't determined target device yet, use this tensor's device
                            if target_device is None and isinstance(materialized, torch.Tensor):
                                target_device = materialized.device
                            materialized_args.append(materialized)
                        else:
                            materialized_args.append(arg)

                    # Move all tensors to the same device if needed
                    if target_device is not None:
                        for i, arg in enumerate(materialized_args):
                            if isinstance(arg, torch.Tensor) and arg.device != target_device:
                                materialized_args[i] = arg.to(target_device)

                    # Call the function with materialized arguments
                    return func(*materialized_args, **kwargs)

            # For operations involving LazyTensors inside capture context, create new LazyTensor
            op_name = cls._normalize_op_name(func)
            
            # ✅ FIX: Infer device from input LazyTensors for operations like vander
            # that don't have device in kwargs
            inferred_device = None
            if 'device' not in kwargs:
                for arg in args:
                    if type(arg).__name__ == 'LazyTensor':
                        # Preserve _original_device string (e.g., remote_accelerator:0)
                        if hasattr(arg, '_original_device') and arg._original_device:
                            orig_dev = arg._original_device
                            if isinstance(orig_dev, str) and 'remote_accelerator' in orig_dev:
                                inferred_device = orig_dev
                            elif isinstance(orig_dev, torch.device) and 'remote_accelerator' in str(orig_dev):
                                inferred_device = str(orig_dev)
                            else:
                                inferred_device = orig_dev
                            break
            
            # ✅ PHASE 2: Try automatic dispatch first (handles ALL operations automatically)
            from .automatic_dispatch import get_automatic_dispatcher
            dispatcher = get_automatic_dispatcher()
            
            if dispatcher.should_use_automatic_dispatch(func, args, kwargs):
                result = dispatcher.dispatch(func, args, kwargs, cls)
                if result is not None:
                    # Automatic dispatch succeeded!
                    logger.debug(f"Automatic dispatch succeeded for {op_name}")
                    return result
                # If automatic dispatch failed, fall through to special handlers
                logger.debug(f"Automatic dispatch failed for {op_name}, trying special handlers")
            
            # ✅ SPECIAL HANDLING: Operations that return tuples (unbind, split, chunk)
            # These need to materialize to avoid infinite recursion
            if hasattr(func, '__name__') and func.__name__ in ('unbind', 'split', 'chunk'):
                # Materialize LazyTensor arguments and call the function
                materialized_args = []
                for arg in args:
                    if type(arg).__name__ == 'LazyTensor':
                        materialized_args.append(arg.materialize())
                    else:
                        materialized_args.append(arg)
                
                # Call the function with materialized arguments
                return func(*materialized_args, **kwargs)
            
            # ✅ SPECIAL HANDLING: scaled_dot_product_attention for CLIP and other models
            # This is PyTorch's optimized attention implementation
            if hasattr(func, '__name__') and func.__name__ == 'scaled_dot_product_attention':
                # Standard attention: (query, key, value) -> output
                # All should have same shape except possibly sequence length
                if len(args) >= 3:
                    try:
                        query, key, value = args[0], args[1], args[2]
                        
                        # Infer output shape (same as query)
                        inferred_shape = query.shape if hasattr(query, 'shape') else torch.Size([])
                        inferred_dtype = query.dtype if hasattr(query, 'dtype') else torch.float32
                        inferred_device = query.device if hasattr(query, 'device') else None
                        
                        # Create new LazyTensor for the attention output
                        from ..semantic.metadata_capture import get_metadata_capture
                        metadata = get_metadata_capture().capture_metadata(
                            operation='aten::scaled_dot_product_attention',
                            inputs=list(args),
                            kwargs=kwargs
                        )
                        
                        return cls(
                            operation='aten::scaled_dot_product_attention',
                            inputs=list(args),
                            kwargs=kwargs,
                            shape=inferred_shape,
                            dtype=inferred_dtype,
                            device=inferred_device,
                            metadata=metadata
                        )
                    except Exception as e:
                        logger.error(f"Failed to create LazyTensor for scaled_dot_product_attention: {e}", exc_info=True)
                        return NotImplemented
            
            # ✅ SPECIAL HANDLING: torch.cat takes a sequence of tensors
            # Handle this specially to support ViT and other models
            if hasattr(func, '__name__') and func.__name__ == 'cat':
                # First argument is a sequence of tensors
                if len(args) > 0 and isinstance(args[0], (list, tuple)):
                    tensors = args[0]
                    # Check if any are LazyTensors
                    has_lazy = any(type(t).__name__ == 'LazyTensor' for t in tensors)
                    
                    if has_lazy:
                        try:
                            # Get dimension (default is 0)
                            dim = kwargs.get('dim', 0) if kwargs else 0
                            if len(args) > 1:
                                dim = args[1]
                            
                            # Use ShapeInference to infer output shape
                            try:
                                inferred_shape = ShapeInference.infer_shape('aten::cat', [tensors], {'dim': dim})
                            except Exception as e:
                                # Fallback: use first tensor's shape
                                logger.debug(f"Shape inference failed for cat: {e}")
                                inferred_shape = tensors[0].shape if hasattr(tensors[0], 'shape') else torch.Size([])
                            
                            # Infer dtype from first tensor
                            inferred_dtype = None
                            for t in tensors:
                                if hasattr(t, 'dtype'):
                                    inferred_dtype = t.dtype
                                    break
                            
                            # Infer device from first tensor
                            inferred_device = None
                            for t in tensors:
                                if hasattr(t, 'device'):
                                    inferred_device = t.device
                                    break
                            
                            # Create new LazyTensor for the concatenation
                            from ..semantic.metadata_capture import get_metadata_capture
                            metadata = get_metadata_capture().capture_metadata(
                                operation='aten::cat',
                                inputs=[tensors],
                                kwargs={'dim': dim}
                            )
                            
                            result = cls(
                                operation='aten::cat',
                                inputs=[tensors],
                                kwargs={'dim': dim},
                                shape=inferred_shape,
                                dtype=inferred_dtype or torch.float32,
                                device=inferred_device,
                                metadata=metadata
                            )
                            logger.debug(f"Created LazyTensor for cat: {result}")
                            return result
                        except Exception as e:
                            logger.error(f"Failed to create LazyTensor for cat: {e}", exc_info=True)
                            return NotImplemented
                # If we reach here, no LazyTensors were found, let PyTorch handle it
                return NotImplemented

            # ✅ NEW: Capture semantic metadata for torch functions too
            from ..semantic.metadata_capture import get_metadata_capture
            metadata = get_metadata_capture().capture_metadata(
                operation=op_name,
                inputs=list(args),
                kwargs=kwargs
            )

            # ✅ NEW: Use ShapeInference for torch functions too
            from .shape_inference import ShapeInference

            try:
                # Extract shapes from tensor arguments for ShapeInference
                input_shapes = []
                positional_args = []

                for arg in args:
                    if isinstance(arg, torch.Tensor):
                        input_shapes.append(arg.shape)
                    elif hasattr(arg, 'shape'):  # LazyTensor
                        input_shapes.append(arg.shape)
                    else:
                        positional_args.append(arg)

                # Call ShapeInference with proper format: (input_shapes,), *positional_args, **kwargs
                inferred_shape = ShapeInference.infer_shape(op_name, tuple(input_shapes), *positional_args, **kwargs)
                if op_name == 'aten::repeat':
                    logger.info(f"🔍 Repeat shape inference: inputs={input_shapes}, args={positional_args}, result={inferred_shape}")
            except Exception as e:
                logger.warning(f"Shape inference failed for {op_name}: {e}", exc_info=True)
                inferred_shape = torch.Size([])
            
            # Infer dtype using ShapeInference
            inferred_dtype = None
            if 'dtype' not in kwargs:
                try:
                    inferred_dtype = ShapeInference.infer_dtype(op_name, list(args), kwargs)
                except Exception:
                    inferred_dtype = torch.float32
            else:
                inferred_dtype = kwargs['dtype']
            
            # Infer device
            inferred_device = None
            try:
                inferred_device = ShapeInference.infer_device(op_name, list(args), kwargs)
            except Exception:
                inferred_device = torch.device('cpu')

            if op_name == 'aten::softmax':
                logger.info(f"🔍 Creating LazyTensor for softmax with shape: {inferred_shape}")
            
            result = cls(
                operation=op_name,
                inputs=list(args),
                kwargs=kwargs,
                shape=inferred_shape,  # ✅ NEW: Inferred shape
                dtype=inferred_dtype,
                device=inferred_device,
                metadata=metadata  # ✅ NEW: Pass semantic metadata
            )
            
            if op_name == 'aten::softmax':
                logger.info(f"🔍 Created LazyTensor for softmax, result.shape: {result.shape}")
            
            return result
        finally:
            _interception_context.context = prev_context

    # ===================================================================
    # FACTORY METHODS
    # ===================================================================

    @classmethod
    def randn(cls, *size, dtype=None, device=None, requires_grad=False):
        """Create random normal LazyTensor."""
        # Handle case where size is passed as torch.Size or tuple (FIX: Simplified)
        if len(size) == 1 and isinstance(size[0], (torch.Size, tuple, list)):
            size = tuple(size[0])
        else:
            size = tuple(size)

        # ✅ FUNDAMENTAL FIX: Use lazy metadata placeholder instead of capturing immediately
        # This defers expensive semantic analysis (stack introspection, pattern matching)
        # to the scheduling phase when full context is available.
        # Impact: 0.88ms → 0.05ms per factory call (17x speedup!)
        metadata = MetadataPlaceholder(
            operation='aten::randn',
            inputs=size,  # Store size for later reference
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad}
        )

        return cls(
            operation='aten::randn',
            inputs=list(size),
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad},
            shape=torch.Size(size),
            dtype=dtype or torch.float32,
            device=device,
            metadata=metadata  # ← Lazy metadata, not computed yet!
        )

    @classmethod
    def tensor(cls, data, dtype=None, device=None, requires_grad=False):
        """Create LazyTensor from data."""
        # For tensor creation from data, we already know the shape and don't need torch.empty
        if hasattr(data, 'shape'):
            shape = torch.Size(data.shape)
        elif hasattr(data, '__len__'):
            # Handle Python sequences (lists, tuples) - compute proper shape
            def get_sequence_shape(seq):
                """Recursively compute shape of nested sequences."""
                if not hasattr(seq, '__len__'):
                    return []
                try:
                    if len(seq) == 0:
                        return [0]
                    # Check if first element is also a sequence
                    first_elem = seq[0]
                    if hasattr(first_elem, '__len__') and not isinstance(first_elem, str):
                        # Nested sequence - recurse
                        inner_shape = get_sequence_shape(first_elem)
                        return [len(seq)] + inner_shape
                    else:
                        # 1D sequence
                        return [len(seq)]
                except TypeError:
                    return []

            shape_list = get_sequence_shape(data)
            shape = torch.Size(shape_list)
        else:
            shape = torch.Size([])
        inferred_dtype = dtype or (data.dtype if hasattr(data, 'dtype') else torch.float32)

        # Store original device before processing
        original_device = device

        # Handle remote devices - use meta for storage
        if device is None:
            device = torch.device('meta')  # Symbolic device (no storage)
        elif isinstance(device, str) and ('remote_accelerator' in device or 'privateuseone' in device):
            # For remote devices, use meta device for storage
            device = torch.device('meta')
        elif isinstance(device, torch.device) and device.type in ('remote_accelerator', 'privateuseone'):
            # Handle torch.device objects for remote devices
            device = torch.device('meta')

        # Create tensor wrapper using official API - avoid torch.empty to prevent interception
        # This is what makes LazyTensor a "real" tensor
        # Use torch.tensor with meta device to create the underlying tensor
        # Convert numpy dtypes to torch dtypes if needed
        torch_dtype = inferred_dtype
        if hasattr(inferred_dtype, 'type') and hasattr(inferred_dtype.type, '__module__'):
            if inferred_dtype.type.__module__ == 'numpy':
                # Simple mapping from numpy dtypes to torch dtypes
                numpy_to_torch_dtype = {
                    'int8': torch.int8,
                    'int16': torch.int16,
                    'int32': torch.int32,
                    'int64': torch.int64,
                    'uint8': torch.uint8,
                    'float16': torch.float16,
                    'float32': torch.float32,
                    'float64': torch.float64,
                    'bool': torch.bool,
                }
                torch_dtype = numpy_to_torch_dtype.get(str(inferred_dtype), torch.float32)

        wrapper = torch.Tensor._make_subclass(
            cls,
            torch.tensor(0, dtype=torch_dtype, device=device).expand(shape),
            require_grad=False  # Disable autograd for now (Phase 2 addition)
        )

        # ✅ FIX: Set logical device abstraction (same as __new__)
        # Store the logical device (what PyTorch expects)
        logical_device = original_device if original_device else torch.device('cpu')
        if isinstance(logical_device, str):
            logical_device = torch.device(logical_device) if 'remote_accelerator' not in logical_device else torch.device('cpu')
        
        object.__setattr__(wrapper, '_logical_device', logical_device)
        object.__setattr__(wrapper, '_physical_device', torch.device('meta'))
        object.__setattr__(wrapper, '_original_device', original_device)

        # Replace the original device in kwargs with the processed device for __init__
        kwargs = {'dtype': dtype, 'device': device, 'requires_grad': requires_grad}
        if kwargs:
            kwargs = kwargs.copy()
            kwargs['device'] = device

        # ✅ NEW: Capture semantic metadata for tensor creation
        from ..semantic.metadata_capture import get_metadata_capture
        metadata = get_metadata_capture().capture_metadata(
            operation='aten::tensor',
            inputs=[data],
            kwargs=kwargs
        )

        # Initialize the wrapper
        wrapper.__init__(
            operation='aten::tensor',
            inputs=[data],
            kwargs=kwargs,
            shape=shape,
            dtype=inferred_dtype,
            device=device,
            metadata=metadata  # ✅ NEW: Pass semantic metadata
        )

        # ✅ FIX: For tensor creation from data, store the known shape immediately
        # This prevents shape inference failures when the shape is already known
        object.__setattr__(wrapper, '_shape', shape)

        return wrapper

    @classmethod
    def as_tensor(cls, data, dtype=None, device=None):
        """Create LazyTensor from data (alias for tensor)."""
        # Create LazyTensor with correct operation name
        shape = torch.Size(data.shape) if hasattr(data, 'shape') else torch.Size([])
        inferred_dtype = dtype or (data.dtype if hasattr(data, 'dtype') else torch.float32)

        # Store original device before processing
        original_device = device

        # Handle remote devices - use meta for storage
        if device is None:
            device = torch.device('meta')  # Symbolic device (no storage)
        elif isinstance(device, str) and ('remote_accelerator' in device or 'privateuseone' in device):
            # For remote devices, use meta device for storage
            device = torch.device('meta')
        elif isinstance(device, torch.device) and device.type in ('remote_accelerator', 'privateuseone'):
            # Handle torch.device objects for remote devices
            device = torch.device('meta')

        # Convert numpy dtypes to torch dtypes if needed
        torch_dtype = inferred_dtype
        if hasattr(inferred_dtype, 'type') and hasattr(inferred_dtype.type, '__module__'):
            if inferred_dtype.type.__module__ == 'numpy':
                # Simple mapping from numpy dtypes to torch dtypes
                numpy_to_torch_dtype = {
                    'int8': torch.int8,
                    'int16': torch.int16,
                    'int32': torch.int32,
                    'int64': torch.int64,
                    'uint8': torch.uint8,
                    'float16': torch.float16,
                    'float32': torch.float32,
                    'float64': torch.float64,
                    'bool': torch.bool,
                }
                torch_dtype = numpy_to_torch_dtype.get(str(inferred_dtype), torch.float32)

        # Create tensor wrapper using official API - avoid torch.empty to prevent interception
        # Use empty with meta device, then expand to target shape
        wrapper = torch.Tensor._make_subclass(
            cls,
            torch.empty(shape, dtype=torch_dtype, device=device),
            require_grad=False  # Disable autograd for now (Phase 2 addition)
        )

        # ✅ FIX: Set logical device abstraction (same as __new__)
        # Store the logical device (what PyTorch expects)
        logical_device = original_device if original_device else torch.device('cpu')
        if isinstance(logical_device, str):
            logical_device = torch.device(logical_device) if 'remote_accelerator' not in logical_device else torch.device('cpu')
        
        object.__setattr__(wrapper, '_logical_device', logical_device)
        object.__setattr__(wrapper, '_physical_device', torch.device('meta'))
        object.__setattr__(wrapper, '_original_device', original_device)

        # Replace the original device in kwargs with the processed device for __init__
        kwargs = {'dtype': dtype, 'device': device}
        if kwargs:
            kwargs = kwargs.copy()
            kwargs['device'] = device

        # ✅ NEW: Capture semantic metadata for as_tensor
        from ..semantic.metadata_capture import get_metadata_capture
        metadata = get_metadata_capture().capture_metadata(
            operation='aten::as_tensor',
            inputs=[data],
            kwargs=kwargs
        )

        # Initialize the wrapper with as_tensor operation
        wrapper.__init__(
            operation='aten::as_tensor',
            inputs=[data],
            kwargs=kwargs,
            shape=shape,
            dtype=torch_dtype,
            device=device,
            metadata=metadata  # ✅ NEW: Pass semantic metadata
        )

        return wrapper

    @classmethod
    def from_numpy(cls, ndarray, dtype=None, device=None):
        """Create LazyTensor from numpy array."""
        # ✅ NEW: Capture semantic metadata for factory functions
        from ..semantic.metadata_capture import get_metadata_capture
        metadata = get_metadata_capture().capture_metadata(
            operation='aten::from_numpy',
            inputs=[ndarray],
            kwargs={'dtype': dtype, 'device': device}
        )

        return cls(
            operation='aten::from_numpy',
            inputs=[ndarray],
            kwargs={'dtype': dtype, 'device': device},
            shape=torch.Size(ndarray.shape),
            dtype=dtype or torch.from_numpy(ndarray).dtype,
            device=device,
            metadata=metadata  # ✅ NEW: Pass semantic metadata
        )

    @classmethod
    def zeros(cls, *size, dtype=None, device=None, requires_grad=False):
        """Create zeros LazyTensor with deferred metadata capture."""
        # Handle size argument variants
        if len(size) == 1 and isinstance(size[0], (torch.Size, tuple, list)):
            size = tuple(size[0])
        else:
            size = tuple(size)

        # Use lazy metadata like randn()
        metadata = MetadataPlaceholder(
            operation='aten::zeros',
            inputs=size,
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad}
        )

        return cls(
            operation='aten::zeros',
            inputs=list(size),
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad},
            shape=torch.Size(size),
            dtype=dtype or torch.float32,
            device=device,
            metadata=metadata
        )

    @classmethod
    def ones(cls, *size, dtype=None, device=None, requires_grad=False):
        """Create ones LazyTensor with deferred metadata capture."""
        # Handle size argument variants
        if len(size) == 1 and isinstance(size[0], (torch.Size, tuple, list)):
            size = tuple(size[0])
        else:
            size = tuple(size)

        # Use lazy metadata like randn()
        metadata = MetadataPlaceholder(
            operation='aten::ones',
            inputs=size,
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad}
        )

        return cls(
            operation='aten::ones',
            inputs=list(size),
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad},
            shape=torch.Size(size),
            dtype=dtype or torch.float32,
            device=device,
            metadata=metadata
        )

    @classmethod
    def empty(cls, *size, dtype=None, device=None, requires_grad=False):
        """Create empty LazyTensor with deferred metadata capture."""
        # Handle size argument variants
        if len(size) == 1 and isinstance(size[0], (torch.Size, tuple, list)):
            size = tuple(size[0])
        else:
            size = tuple(size)

        # Use lazy metadata like randn()
        metadata = MetadataPlaceholder(
            operation='aten::empty',
            inputs=size,
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad}
        )

        return cls(
            operation='aten::empty',
            inputs=list(size),
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad},
            shape=torch.Size(size),
            dtype=dtype or torch.float32,
            device=device,
            metadata=metadata
        )
    
    @classmethod
    def randint(cls, low, high=None, size=None, dtype=None, device=None, requires_grad=False):
        """Create random integer LazyTensor with deferred metadata capture."""
        # Handle different call signatures:
        # torch.randint(high, size) - low defaults to 0
        # torch.randint(low, high, size)
        if high is None:
            # Single argument: torch.randint(high, size=...)
            high = low
            low = 0
        
        # Handle size argument
        if size is None:
            size = tuple()
        elif isinstance(size, (torch.Size, tuple, list)):
            size = tuple(size)
        else:
            size = (size,)
        
        # Use lazy metadata
        metadata = MetadataPlaceholder(
            operation='aten::randint',
            inputs=(low, high, size),
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad}
        )
        
        return cls(
            operation='aten::randint',
            inputs=[low, high, size],
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad},
            shape=torch.Size(size),
            dtype=dtype or torch.int64,  # ✅ randint defaults to int64
            device=device,
            metadata=metadata
        )

    @classmethod
    def full(cls, fill_value, *size, dtype=None, device=None, requires_grad=False):
        """Create LazyTensor filled with a scalar value."""
        # Handle case where size is passed as torch.Size or tuple (FIX: Simplified)
        if len(size) == 1 and isinstance(size[0], (torch.Size, tuple, list)):
            size = tuple(size[0])
        else:
            size = tuple(size)

        return cls(
            operation='aten::full',
            inputs=[fill_value, *size],
            kwargs={'dtype': dtype, 'device': device, 'requires_grad': requires_grad},
            shape=torch.Size(size),
            dtype=dtype or torch.float32,
            device=device
        )

    # ===================================================================
    # MATERIALIZATION
    # ===================================================================

    def materialize(self) -> torch.Tensor:
        """
        Force execution of the computation graph.

        This traverses the DAG and executes operations to produce
        a concrete tensor.

        Returns:
            Concrete torch.Tensor with actual data
        """
        # ✅ NEW: Ensure runtime is initialized (triggers auto-init if needed)
        from ...backend.runtime.initialization import ensure_initialized, get_runtime_state
        ensure_initialized()
        
        # ✅ NEW: Check if remote execution should be used
        runtime_state = get_runtime_state()
        if runtime_state.coordinator and runtime_state.server_address:
            # Remote execution path
            logger.info(f"🌐 Attempting remote execution on server: {runtime_state.server_address}")
            try:
                return self._materialize_remote()
            except Exception as e:
                logger.warning(f"❌ Remote execution failed: {e}, falling back to local execution")
                logger.info("🏠 Using local execution (remote execution failed)")
                return self._materialize_local()
        else:
            # Local execution path (no remote configuration)
            if hasattr(self, 'device') and str(self.device).startswith('remote_accelerator'):
                logger.warning("⚠️  Remote accelerator device detected but no remote server configured. Using local execution.")
                logger.info("🏠 Using local execution (remote server not configured)")
            else:
                logger.debug("🏠 Using local execution")
            return self._materialize_local()

    def _materialize_local(self) -> torch.Tensor:
        """
        Materialize locally using optimized execution pipeline.

        Week 3 Integration: Uses MaterializationOptimizer for:
        - Topological sort for batch execution
        - CUDA streams for pipelining
        - Pinned memory for faster transfers
        - Reduced Python overhead

        Falls back to graph builder if optimization fails.
        """
        try:
            # ✅ Week 3: Use MaterializationOptimizer for optimized local execution
            from ...server.materialization_optimizer import MaterializationOptimizer
            from ...server.executor import _executor

            optimizer = MaterializationOptimizer(
                enable_pinned_memory=True,
                enable_streams=True
            )

            logger.debug("Using MaterializationOptimizer for local execution")
            return optimizer.execute_optimized(self, _executor)

        except Exception as e:
            logger.warning(f"MaterializationOptimizer failed: {e}, falling back to graph builder")
            # Fallback to original implementation
            from .graph_builder import get_global_builder
            builder = get_global_builder()
            return builder.materialize(self)

    def _materialize_remote(self) -> torch.Tensor:
        """
        Execute remotely using subgraph optimization.

        KEY CHANGE: Send entire computation DAG instead of single operation!
        This reduces O(n) network round-trips to O(1).
        """
        from ...backend.runtime.initialization import get_runtime_state
        from ...server.smart_subgraph_builder import SmartSubgraphBuilder, FragmentationConfig
        from ...server.subgraph_cache import get_subgraph_cache
        import asyncio

        state = get_runtime_state()
        coordinator = state.coordinator

        if not coordinator:
            logger.warning("No coordinator available, falling back to local execution")
            return self._materialize_local()

        try:
            # ✅ NEW: Use cached subgraph builder
            builder = SmartSubgraphBuilder(
                FragmentationConfig(
                    memory_limit_gb=8.0,
                    network_gbps=100.0,
                    prefer_local_compute=False  # We want remote execution
                )
            )

            # ✅ Get or build subgraph (with caching!)
            cache = get_subgraph_cache()
            subgraph = cache.get_or_build(self, builder)

            logger.info(f"🚀 Subgraph ready: {len(subgraph.operations)} ops, "
                       f"{len(subgraph.input_tensors)} inputs")

            # Prepare input data (materialize external inputs)
            input_data = {}
            for tensor_id, tensor in subgraph.input_tensors.items():
                if isinstance(tensor, LazyTensor):
                    # Materialize factory operations locally
                    if self._is_factory_operation(tensor):
                        materialized = self._materialize_factory_op(tensor)
                        input_data[str(tensor_id)] = materialized
                    else:
                        # This shouldn't happen if builder works correctly
                        logger.warning(f"Non-factory external input: {tensor.operation}")
                        input_data[str(tensor_id)] = tensor._materialize_local()
                else:
                    input_data[str(tensor_id)] = tensor

            # Send entire subgraph for execution
            # ✅ Week 3: Enable differential updates for iterative workloads
            graph_id = f"subgraph_{id(self)}"  # Use tensor ID as graph identifier

            # Define async execution function
            async def execute_subgraph_async():
                return await coordinator.execute_remote_subgraph(
                    subgraph=subgraph.serialize(),
                    input_data=input_data,
                    target=state.server_address,
                    timeout=30,
                    graph_id=graph_id,  # Enable differential protocol
                    enable_differential=True
                )

            # Run async operation
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                # No running loop, create new one
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    result = loop.run_until_complete(execute_subgraph_async())
                    return result
                finally:
                    loop.close()
            else:
                # There's already a running loop, we can't use run_until_complete
                # This shouldn't happen in normal usage, but handle it gracefully
                logger.warning("Nested event loop detected, falling back to local execution")
                return self._materialize_local()

        except Exception as e:
            logger.warning(f"Subgraph execution failed: {e}, falling back to local", exc_info=True)
            return self._materialize_local()

    def _is_factory_operation(self, tensor: LazyTensor) -> bool:
        """Check if tensor represents a factory operation (creates data from scratch)."""
        factory_ops = {
            'aten::randn', 'aten::zeros', 'aten::ones', 'aten::empty',
            'aten::tensor', 'aten::as_tensor', 'aten::from_numpy'
        }
        return tensor.operation in factory_ops

    def _materialize_factory_op(self, tensor: LazyTensor) -> torch.Tensor:
        """
        Materialize a factory operation locally.

        Factory operations create tensors from scratch, so we can execute them
        locally without needing the remote GPU.
        """
        try:
            # Execute the factory operation locally
            op_name = tensor.operation.split('::')[1]

            # Use torch functions directly instead of torch.ops.aten
            if op_name == 'randn':
                # torch.randn(size, ...)
                result = torch.randn(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'zeros':
                # torch.zeros(size, ...)
                result = torch.zeros(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'ones':
                # torch.ones(size, ...)
                result = torch.ones(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'empty':
                # torch.empty(size, ...)
                result = torch.empty(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'full':
                # torch.full(size, fill_value, ...)
                result = torch.full(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'tensor':
                # torch.tensor(data, ...)
                result = torch.tensor(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'as_tensor':
                # torch.as_tensor(data, ...)
                result = torch.as_tensor(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'from_numpy':
                # torch.from_numpy(data, ...)
                result = torch.from_numpy(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'eye':
                # torch.eye(n, ...)
                result = torch.eye(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'arange':
                # torch.arange(start, end, ...)
                result = torch.arange(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'linspace':
                # torch.linspace(start, end, ...)
                result = torch.linspace(*tensor.inputs, **tensor.kwargs)
            elif op_name == 'logspace':
                # torch.logspace(start, end, ...)
                result = torch.logspace(*tensor.inputs, **tensor.kwargs)
            else:
                # Fallback to torch.ops.aten
                op_func = getattr(torch.ops.aten, op_name)
                result = op_func(*tensor.inputs, **tensor.kwargs)

            logger.debug(f"Materialized factory op {tensor.operation} locally")
            return result

        except Exception as e:
            logger.warning(f"Failed to materialize factory op {tensor.operation}: {e}")
            # Fallback to local DAG execution
            return tensor._materialize_local()


    # Operations that force materialization
    _MATERIALIZATION_OPS = {
        torch.Tensor.cpu,
        torch.Tensor.cuda,
        torch.Tensor.numpy,
        torch.Tensor.item,
        torch.Tensor.tolist,
        torch.Tensor.__bool__,
        torch.Tensor.__int__,
        torch.Tensor.__float__,
        torch.Tensor.__len__,
        # Comparison operations that need concrete values
        torch.Tensor.__gt__,
        torch.Tensor.__ge__,
        torch.Tensor.__lt__,
        torch.Tensor.__le__,
        torch.Tensor.__eq__,
        torch.Tensor.__ne__,
    }

    # ===================================================================
    # SHAPE INFERENCE
    # ===================================================================

    @classmethod
    def _infer_shape(
        cls,
        operation: str,
        inputs: List[Any],
        kwargs: Dict[str, Any]
    ) -> Optional[torch.Size]:
        """
        Infer output shape using PyTorch's meta tensor system.

        This executes the operation on "fake" tensors that only track
        shape/dtype without allocating storage.
        """
        # ✅ WEEK 2 OPTIMIZATION: Pattern-based shape cache
        # For common transformer operations, use precomputed patterns
        pattern_name = cls._match_shape_pattern(operation, inputs)
        if pattern_name:
            try:
                return cls._apply_shape_pattern(pattern_name, inputs)
            except Exception as e:
                logger.debug(f"Pattern shape inference failed for {pattern_name}: {e}")
                # Fall through to meta tensor inference
        
        # Fast path for simple operations (avoids FakeTensorMode overhead)
        if operation in LazyTensor._SIMPLE_OPS:
            try:
                return LazyTensor._SIMPLE_OPS[operation](inputs)
            except:
                pass  # Fall through to FakeTensorMode

        # Use cached shape inference with LazyTensor IDs as keys
        return LazyTensor._infer_shape_with_cache(operation, inputs, kwargs)

    @classmethod
    def _infer_shape_with_cache(cls, operation: str, inputs: List[Any], kwargs: Dict[str, Any]) -> Optional[torch.Size]:
        """
        Cached shape inference with bounded global cache + thread-local LRU.

        PHASE 2 OPTIMIZATION: Use process-wide cache for better hit rates
        while maintaining thread safety with lock-free fast path.

        Strategy:
        1. Check global cache (fast path, no lock needed)
        2. If miss, compute shape
        3. Store in global cache with lock-based eviction

        See: docs/optimization_guide.md - Problem 1: Graph Capture Overhead
        """
        # Build cache signature from operation and input shapes/types
        inputs_signature = cls._build_inputs_signature(inputs)
        cache_key = f"{operation}|{inputs_signature}"
        
        # Try global cache first (fast path, lock-free)
        def compute_shape():
            # Fallback to FakeTensorMode computation
            return cls._infer_shape_original(operation, inputs, kwargs)
        
        # Use global cache with automatic eviction
        result = _get_or_cache_shape(cache_key, compute_shape)
        return result

    @staticmethod
    def _build_inputs_signature(inputs: List[Any]) -> str:
        """Build a hashable signature of inputs for caching."""
        parts = []
        for inp in inputs:
            if isinstance(inp, LazyTensor):
                # Use shape + dtype for LazyTensors
                parts.append(f"lt_{tuple(inp.shape) if inp.shape else 'none'}_{inp.dtype}")
            elif isinstance(inp, torch.Tensor):
                # Use shape + dtype for concrete tensors
                parts.append(f"t_{tuple(inp.shape)}_{inp.dtype}")
            else:
                # Scalar or non-tensor - use string representation
                parts.append(f"s_{repr(inp)}")

        return "|".join(parts)

    @staticmethod
    def _infer_shape_original(
        operation: str,
        inputs: List[Any],
        kwargs: Dict[str, Any]
    ) -> Optional[torch.Size]:
        """
        Original shape inference logic using FakeTensor.
        
        This method is called after cache miss to compute actual shapes.
        """
        try:
            # ✅ FIX: Reuse thread-local FakeTensorMode to avoid recreation overhead
            with _get_thread_local_fake_mode():
                # Convert inputs to fake tensors
                fake_inputs = []
                for inp in inputs:
                    if isinstance(inp, LazyTensor):
                        # CRITICAL FIX: Use object.__getattribute__ to bypass property recursion
                        inp_shape = object.__getattribute__(inp, '_shape')
                        inp_dtype = object.__getattribute__(inp, '_dtype')

                        # Guard: if shape can't be inferred, return None early
                        if inp_shape is None:
                            logger.warning(f"Can't infer shape for {inp.operation} (unsupported op)")
                            return None

                        # ✅ FIX: Check for empty shape placeholder (uninitialized shape)
                        # Empty shape torch.Size([]) means "not inferred yet" for non-tensor operations
                        if len(inp_shape) == 0 and inp.operation not in ('aten::tensor', 'aten::scalar'):
                            logger.warning(
                                f"Cannot infer shape for {operation}: "
                                f"input operation {inp.operation} has uninitialized shape (empty placeholder)"
                            )
                            return None

                        fake_inputs.append(
                            torch.empty(inp_shape, dtype=inp_dtype or torch.float32, device='meta')
                        )
                    elif isinstance(inp, torch.Tensor):
                        # Convert to meta tensor (ensure consistent device)
                        fake_inputs.append(inp.to('meta'))
                    else:
                        # Scalar or non-tensor (keep as-is)
                        fake_inputs.append(inp)

                # Get operation function
                op_func = LazyTensor._get_operation_function(operation)

                # Execute on fake tensors (shape inference only)
                fake_result = op_func(*fake_inputs, **kwargs)

                # Extract shape
                if isinstance(fake_result, torch.Tensor):
                    return fake_result.shape
                else:
                    return torch.Size([])

        except Exception as e:
            logger.debug(f"Shape inference failed for {operation}: {e}")
            # Fallback: try simple heuristics
            return LazyTensor._infer_shape_fallback(operation, inputs)

    @staticmethod
    def _infer_shape_fallback(
        operation: str,
        inputs: List[Any]
    ) -> Optional[torch.Size]:
        """
        Fallback shape inference using simple heuristics.

        Used when FakeTensorMode fails (dynamic shapes, unsupported ops, etc.)
        """
        # Element-wise operations preserve shape
        if operation in ['aten::relu', 'aten::sigmoid', 'aten::tanh',
                        'aten::abs', 'aten::neg', 'aten::exp', 'aten::log']:
            if inputs and hasattr(inputs[0], 'shape'):
                if isinstance(inputs[0], LazyTensor):
                    # Use metadata directly to avoid recursion
                    try:
                        return object.__getattribute__(inputs[0], '_shape')
                    except AttributeError:
                        # Fallback if metadata not set yet
                        return torch.Size([])
                else:
                    return inputs[0].shape

        # Matrix multiplication
        if operation in ['aten::matmul', 'aten::mm']:
            if len(inputs) >= 2:
                a_shape = None
                b_shape = None

                if hasattr(inputs[0], 'shape'):
                    if isinstance(inputs[0], LazyTensor):
                        try:
                            a_shape = object.__getattribute__(inputs[0], '_shape')
                        except AttributeError:
                            a_shape = torch.Size([])
                    else:
                        a_shape = inputs[0].shape

                if hasattr(inputs[1], 'shape'):
                    if isinstance(inputs[1], LazyTensor):
                        try:
                            b_shape = object.__getattribute__(inputs[1], '_shape')
                        except AttributeError:
                            b_shape = torch.Size([])
                    else:
                        b_shape = inputs[1].shape

                if a_shape and b_shape and len(a_shape) >= 2 and len(b_shape) >= 2:
                    # (..., M, K) @ (..., K, N) -> (..., M, N)
                    return torch.Size([*a_shape[:-1], b_shape[-1]])

        # Broadcasting operations
        if operation in ['aten::add', 'aten::sub', 'aten::mul', 'aten::div']:
            if len(inputs) >= 2:
                a_shape = None
                b_shape = None

                if hasattr(inputs[0], 'shape'):
                    if isinstance(inputs[0], LazyTensor):
                        try:
                            a_shape = object.__getattribute__(inputs[0], '_shape')
                        except AttributeError:
                            a_shape = torch.Size([])
                    else:
                        a_shape = inputs[0].shape

                if hasattr(inputs[1], 'shape'):
                    if isinstance(inputs[1], LazyTensor):
                        try:
                            b_shape = object.__getattribute__(inputs[1], '_shape')
                        except AttributeError:
                            b_shape = torch.Size([])
                    else:
                        b_shape = inputs[1].shape

                if a_shape and b_shape:
                    # Simple broadcasting (return larger shape)
                    if len(a_shape) >= len(b_shape):
                        return a_shape
                    else:
                        return b_shape

        # Unknown - return empty shape
            return None

    @staticmethod
    def _infer_dtype(inputs: List[Any], kwargs: Dict[str, Any]) -> Optional[torch.dtype]:
        """Infer output dtype from inputs or kwargs."""
        # Explicit dtype in kwargs
        if 'dtype' in kwargs and kwargs['dtype'] is not None:
            dtype = kwargs['dtype']
            # Convert numpy dtypes to PyTorch dtypes
            if hasattr(dtype, 'type') and hasattr(dtype.type, '__module__'):
                if dtype.type.__module__ == 'numpy':
                    # Convert numpy dtype to PyTorch dtype
                    if dtype == torch.float32.numpy_dtype():
                        return torch.float32
                    elif dtype == torch.float64.numpy_dtype():
                        return torch.float64
                    elif dtype == torch.int64.numpy_dtype():
                        return torch.int64
                    elif dtype == torch.int32.numpy_dtype():
                        return torch.int32
                    # Add more conversions as needed
                    else:
                        # Fallback: try to map by name
                        dtype_name = str(dtype).split('.')[-1].replace('Dtype', '').lower()
                        dtype_map = {
                            'float32': torch.float32,
                            'float64': torch.float64,
                            'int32': torch.int32,
                            'int64': torch.int64,
                            'bool': torch.bool,
                        }
                        return dtype_map.get(dtype_name, torch.float32)
            return dtype

        # Infer from tensor inputs (LazyTensor or concrete tensors)
        for inp in inputs:
            if isinstance(inp, LazyTensor):
                # Use metadata directly to avoid recursion
                try:
                    inp_dtype = object.__getattribute__(inp, '_dtype')
                    if inp_dtype is not None:
                        return inp_dtype
                except AttributeError:
                    pass

                # If LazyTensor doesn't have dtype set, try to infer from its inputs
                # This handles cases where dtype wasn't set during construction
                inp_inputs = object.__getattribute__(inp, '_inputs')
                if inp_inputs:
                    # Recursively infer from inputs (but avoid infinite recursion)
                    try:
                        inferred = cls._infer_dtype(inp_inputs, object.__getattribute__(inp, '_kwargs') or {})
                        if inferred is not None:
                            return inferred
                    except:
                        pass

                # Fallback to default if metadata not set yet
                return torch.float32
            elif isinstance(inp, torch.Tensor):
                return inp.dtype

        # If no tensor inputs found, try to infer from scalar values that can be converted to tensors
        # This handles cases like x + 1.0 where 1.0 becomes a scalar tensor
        for inp in inputs:
            if isinstance(inp, (int, float)):
                # Default to float32 for scalar arithmetic
                return torch.float32
            elif hasattr(inp, 'dtype'):  # Handle other tensor-like objects
                return inp.dtype

        # Default
        return None

    # ===================================================================
    # UTILITIES
    # ===================================================================

    @staticmethod
    def _normalize_op_name(func) -> str:
        """
        Normalize operation names to canonical form.

        Examples:
            torch.ops.aten.add.Tensor -> aten::add
            torch.add -> aten::add
            add -> aten::add
            <method 'cpu' of ...> -> aten::cpu
        """
        # Debug logging for problematic cases
        if hasattr(func, '__name__') and func.__name__ == '__get__':
            logger.debug(f"Normalizing __get__ method: {func}")

        if hasattr(func, '__name__'):
            name = func.__name__
        elif hasattr(func, '_schema'):
            # ATen operation with schema
            schema_str = str(func._schema)
            name = schema_str.split('(')[0]
            if '::' in name:
                name = name.split('::')[-1]
        else:
            # Handle method objects (e.g., <method 'cpu' of ...>)
            func_str = str(func)
            if "'method '" in func_str and " of " in func_str:
                # Extract method name from string representation
                method_part = func_str.split("'method '")[1].split("'")[0]
                name = method_part
            else:
                name = str(func)

        # Remove overload suffix (e.g., add.Tensor -> add)
        name = name.split('.')[0]

        # Handle special cases for method names that start with __
        if name.startswith('__') and name.endswith('__'):
            # This is likely a dunder method, map to the actual operation
            if name == '__len__':
                name = 'len'
            elif name == '__add__':
                name = 'add'
            elif name == '__matmul__':
                name = 'matmul'
            elif name == '__sub__':
                name = 'sub'
            elif name == '__mul__':
                name = 'mul'
            elif name == '__truediv__':
                name = 'div'
            elif name == '__floordiv__':
                name = 'floordiv'
            elif name == '__mod__':
                name = 'remainder'
            elif name == '__pow__':
                name = 'pow'
            elif name == '__lshift__':
                name = 'lshift'
            elif name == '__rshift__':
                name = 'rshift'
            elif name == '__and__':
                name = 'bitwise_and'
            elif name == '__or__':
                name = 'bitwise_or'
            elif name == '__xor__':
                name = 'bitwise_xor'
            elif name == '__invert__':
                name = 'bitwise_not'
            elif name == '__lt__':
                name = 'lt'
            elif name == '__le__':
                name = 'le'
            elif name == '__gt__':
                name = 'gt'
            elif name == '__ge__':
                name = 'ge'
            elif name == '__eq__':
                name = 'eq'
            elif name == '__ne__':
                name = 'ne'
            else:
                # For other dunder methods that don't have direct aten equivalents,
                # especially internal methods like __get__, __set__, etc.
                # These are internal Python methods that shouldn't be intercepted
                logger.debug(f"Ignoring internal dunder method: {name} for func: {func}")
                name = 'unknown_method'

        # Ensure aten:: prefix
        if not name.startswith('aten::'):
            name = f'aten::{name}'

        return name

    @staticmethod
    def _get_operation_function(operation: str):
        """
        Get PyTorch function for an operation name.

        Maps "aten::add" -> torch.ops.aten.add
        """
        if operation.startswith('aten::'):
            op_name = operation[6:]  # Remove "aten::" prefix
            try:
                return getattr(torch.ops.aten, op_name)
            except AttributeError:
                # Fallback to torch namespace
                return getattr(torch, op_name)
        else:
            return getattr(torch, operation)

    # ===================================================================
    # PROPERTY ACCESSORS
    # ===================================================================

    @property
    def operation(self) -> str:
        """Get the operation that created this tensor."""
        return object.__getattribute__(self, '_operation')

    @property
    def inputs(self) -> List[Any]:
        """Get the input arguments to this operation."""
        return object.__getattribute__(self, '_inputs')

    @property
    def kwargs(self) -> Dict[str, Any]:
        """Get the keyword arguments to this operation."""
        return object.__getattribute__(self, '_kwargs')

    @property
    def tensor_id(self) -> int:
        """Get unique ID for this tensor."""
        return object.__getattribute__(self, '_tensor_id')

    @property
    def id(self) -> str:
        """Get unique ID for this tensor (alias for tensor_id)."""
        return str(self.tensor_id)

    @property
    def metadata(self) -> Dict[str, Any]:
        """
        Lazy metadata computation.

        Only computes when accessed (e.g., during scheduling).
        Not computed during capture!
        """
        # Check if already computed (cached)
        cached_metadata = object.__getattribute__(self, '_metadata')
        if cached_metadata is not None:
            return cached_metadata

        # Compute now (only once) - LAZY COMPUTATION
        from ..semantic.metadata_capture import get_metadata_capture
        try:
            computed_metadata = get_metadata_capture().capture_metadata(
                operation=self.operation,
                inputs=self.inputs,
                kwargs=self.kwargs
            )
            # Cache the result
            object.__setattr__(self, '_metadata', computed_metadata)
            return computed_metadata
        except Exception as e:
            logger.debug(f"Metadata capture failed for {self.operation}: {e}")
            # Cache empty dict to avoid repeated failures
            empty_metadata = {}
            object.__setattr__(self, '_metadata', empty_metadata)
            return empty_metadata


    @property
    def shape(self) -> torch.Size:
        """
        Lazy shape computation.

        Only computes when accessed, not during capture!
        Most operations never access shape during capture.
        """
        # Check if already computed (cached)
        cached_shape = object.__getattribute__(self, '_shape')
        if cached_shape is not None:
            # Ensure cached shape is torch.Size
            if isinstance(cached_shape, torch.Size):
                return cached_shape
            elif isinstance(cached_shape, (tuple, list)):
                # Convert to torch.Size and cache it
                cached_shape = torch.Size(cached_shape)
                object.__setattr__(self, '_shape', cached_shape)
                return cached_shape
            else:
                # Invalid cached shape, clear it and recompute
                object.__setattr__(self, '_shape', None)

        # ✅ FIX: Use pre-computed _inferred_shape if available (from constructor)
        # This avoids circular dependencies when ShapeInference tries to access input shapes
        inferred_shape = object.__getattribute__(self, '_inferred_shape')
        if inferred_shape is not None:
            # Ensure it's a torch.Size, not a LazyTensor or other type
            if isinstance(inferred_shape, (tuple, list)):
                inferred_shape = torch.Size(inferred_shape)
            elif not isinstance(inferred_shape, torch.Size):
                # Safety: convert to torch.Size
                inferred_shape = torch.Size(inferred_shape) if hasattr(inferred_shape, '__iter__') else torch.Size([])
            # Cache the result
            object.__setattr__(self, '_shape', inferred_shape)
            return inferred_shape

        # Fallback: Compute now (only once) - LAZY COMPUTATION
        from .shape_inference import ShapeInference
        try:
            inferred_shape = ShapeInference.infer_shape(self.operation, self.inputs, self.kwargs)
            if inferred_shape is not None:
                # Ensure it's a torch.Size
                if isinstance(inferred_shape, (tuple, list)):
                    inferred_shape = torch.Size(inferred_shape)
                elif not isinstance(inferred_shape, torch.Size):
                    inferred_shape = torch.Size(inferred_shape) if hasattr(inferred_shape, '__iter__') else torch.Size([])
                # Cache the result
                object.__setattr__(self, '_shape', inferred_shape)
                return inferred_shape
        except Exception as e:
            logger.debug(f"Shape inference failed for {self.operation}: {e}")

        # Fallback to empty shape
        fallback_shape = torch.Size([])
        object.__setattr__(self, '_shape', fallback_shape)
        return fallback_shape

    @property
    def dtype(self) -> torch.dtype:
        """Get the dtype of this tensor."""
        # Prevent recursion when accessing dtype
        from .interception_control import get_current_context, InterceptionContext
        if get_current_context() != InterceptionContext.NONE:
            return object.__getattribute__(self, '_dtype') or torch.float32

        return object.__getattribute__(self, '_dtype') or torch.float32

    @property
    def device(self):
        """
        Get the device of this tensor.
        
        ✅ PHASE 1: Returns logical device (what PyTorch expects).
        The physical device is always 'meta' (no storage).
        """
        # Prevent recursion when accessing device
        from .interception_control import get_current_context, InterceptionContext
        if get_current_context() != InterceptionContext.NONE:
            # Try logical device first (new abstraction)
            logical_device = object.__getattribute__(self, '_logical_device')
            if logical_device is not None:
                return logical_device
            
            # Fallback to original_device for backward compatibility
            original_device = object.__getattribute__(self, '_original_device')
            if original_device is not None:
                # For remote devices, return a torch.device with privateuseone type
                if isinstance(original_device, str) and 'remote_accelerator' in original_device:
                    return torch.device('privateuseone:0')
                return original_device
            return object.__getattribute__(self, '_device') or torch.device('meta')

        # Try logical device first (new abstraction)
        logical_device = object.__getattribute__(self, '_logical_device')
        if logical_device is not None:
            return logical_device
        
        # Fallback to original_device for backward compatibility
        original_device = object.__getattribute__(self, '_original_device')
        if original_device is not None:
            # For remote devices, return a torch.device with privateuseone type
            if isinstance(original_device, str) and 'remote_accelerator' in original_device:
                return torch.device('privateuseone:0')
            return original_device
        return object.__getattribute__(self, '_device') or torch.device('meta')

    @property
    def original_device(self):
        """Get the original device (before meta conversion)."""
        return object.__getattribute__(self, '_original_device')
    
    # ===================================================================
    # ADDITIONAL METADATA PROPERTIES (for local queries without remote calls)
    # ===================================================================
    
    def size(self, dim: Optional[int] = None):
        """
        Return the size of the tensor.
        
        Args:
            dim: If specified, return size of that dimension. Otherwise return full size.
            
        Returns:
            torch.Size or int
        """
        shape = self.shape
        if dim is None:
            return shape
        
        # Check if shape is empty
        if shape is None:
            raise IndexError(f"Cannot access dimension {dim} of tensor with empty shape. "
                           f"LazyTensor(op={self.operation}) has uninitialized shape. "
                           f"This usually means shape inference failed for this operation.")
        
        # Normalize negative dimensions first (before scalar check)
        # Ensure shape is a tuple/torch.Size before calling len()
        if not isinstance(shape, (tuple, torch.Size)):
            raise TypeError(f"Expected shape to be tuple or torch.Size, got {type(shape)}")
        
        # Handle scalar tensors (shape = ())
        if len(shape) == 0:
            if dim is not None:
                # Try materializing to get actual shape (shape inference might be wrong)
                try:
                    materialized = self.materialize()
                    actual_shape = materialized.shape
                    if len(actual_shape) > 0:
                        # Shape inference was wrong - use actual shape
                        if dim < 0:
                            dim = len(actual_shape) + dim
                        if 0 <= dim < len(actual_shape):
                            return actual_shape[dim]
                except Exception:
                    # Materialization failed, use inferred shape
                    pass
                
                # Normalize negative dimension for scalar (though it will still be invalid)
                if dim < 0:
                    dim = len(shape) + dim  # This will be -1 + 0 = -1, still invalid
                raise IndexError(f"Dimension out of range for scalar tensor with dim={dim}. "
                               f"LazyTensor(op={self.operation}) has scalar shape. "
                               f"This usually indicates incorrect shape inference.")
            return shape
        
        # Normalize negative dimensions for non-scalar tensors
        if dim < 0:
            dim = len(shape) + dim
        
        # Bounds check
        if dim < 0 or dim >= len(shape):
            raise IndexError(f"Dimension out of range (expected to be in range of [{-len(shape)}, {len(shape)-1}], "
                           f"but got {dim}). Shape: {shape}")
        
        return shape[dim]
    
    def dim(self) -> int:
        """Return the number of dimensions."""
        return len(self.shape)
    
    @property
    def ndim(self) -> int:
        """Return the number of dimensions (alias for dim())."""
        return self.dim()
    
    def numel(self) -> int:
        """Return the total number of elements in the tensor."""
        import math
        return math.prod(self.shape)
    
    @property
    def is_cuda(self) -> bool:
        """Check if tensor is on CUDA device."""
        device = self.device
        return device.type == 'cuda' if device else False
    
    @property
    def is_cpu(self) -> bool:
        """Check if tensor is on CPU."""
        device = self.device
        return device.type == 'cpu' if device else False
    
    @property
    def requires_grad(self) -> bool:
        """Check if tensor requires gradient."""
        return object.__getattribute__(self, '_requires_grad') if hasattr(self, '_requires_grad') else False
    
    @property
    def is_leaf(self) -> bool:
        """Check if tensor is a leaf node in autograd graph."""
        return object.__getattribute__(self, '_is_leaf') if hasattr(self, '_is_leaf') else True
    
    @property
    def grad_fn(self):
        """Get gradient function (for autograd)."""
        # grad_fn is a special PyTorch attribute - return None for LazyTensor
        return None
    
    def is_contiguous(self, memory_format=torch.contiguous_format) -> bool:
        """
        Check if tensor is contiguous in memory.
        
        For LazyTensor, we assume contiguous by default since we haven't
        materialized yet. This can be refined later.
        """
        return object.__getattribute__(self, '_is_contiguous') if hasattr(self, '_is_contiguous') else True
    
    def stride(self, dim: Optional[int] = None):
        """
        Return the stride of the tensor.
        
        For LazyTensor, compute stride from shape assuming contiguous layout.
        """
        shape = self.shape
        if not shape:
            return () if dim is None else 1
        
        # Compute contiguous strides
        strides = []
        stride = 1
        for s in reversed(shape):
            strides.append(stride)
            stride *= s
        strides.reverse()
        
        if dim is None:
            return tuple(strides)
        
        # Normalize negative dimension
        if dim < 0:
            dim = len(shape) + dim
        return strides[dim]
    
    def is_floating_point(self) -> bool:
        """Check if tensor has floating point dtype."""
        dtype = self.dtype
        return dtype in (torch.float16, torch.float32, torch.float64, torch.bfloat16)
    
    def is_complex(self) -> bool:
        """Check if tensor has complex dtype."""
        dtype = self.dtype
        return dtype in (torch.complex32, torch.complex64, torch.complex128)
    
    def is_signed(self) -> bool:
        """Check if tensor has signed dtype."""
        dtype = self.dtype
        # All floating point and complex types are signed
        if self.is_floating_point() or self.is_complex():
            return True
        # Check integer types
        return dtype in (torch.int8, torch.int16, torch.int32, torch.int64)
    
    def get_device(self) -> int:
        """Get device index (-1 for CPU)."""
        device = self.device
        if device.type == 'cpu':
            return -1
        return device.index if device.index is not None else 0
    
    def type(self, dtype=None):
        """
        Return the type string or cast to a new type.
        
        Args:
            dtype: If specified, cast to this type. Otherwise return type string.
        """
        if dtype is None:
            # Return type string
            device_str = 'cuda' if self.is_cuda else 'cpu'
            dtype_str = str(self.dtype).replace('torch.', '')
            return f'torch.{device_str}.{dtype_str}'
        else:
            # Cast to new type (create new LazyTensor with cast operation)
            return self.to(dtype=dtype)
    
    def __len__(self) -> int:
        """Return the size of the first dimension."""
        shape = self.shape
        if not shape:
            raise TypeError("len() of a 0-d tensor")
        return shape[0]

    def view(self, *shape):
        """
        Return a new tensor with the same data but different shape.
        
        This method is critical for model compatibility (e.g., GPT2).
        """
        # Handle different input formats
        if len(shape) == 1:
            if isinstance(shape[0], (torch.Size, tuple, list)):
                shape = tuple(shape[0])
            else:
                shape = (shape[0],)
        
        # Use torch.reshape which goes through __torch_dispatch__
        return torch.reshape(self, shape)
    
    # ===================================================================
    # TYPE CONVERSION METHODS
    # ===================================================================
    
    def long(self):
        """Convert to long (int64) dtype."""
        return self.to(dtype=torch.long)
    
    def float(self):
        """Convert to float (float32) dtype."""
        return self.to(dtype=torch.float)
    
    def double(self):
        """Convert to double (float64) dtype."""
        return self.to(dtype=torch.double)
    
    def int(self):
        """Convert to int (int32) dtype."""
        return self.to(dtype=torch.int32)
    
    def half(self):
        """Convert to half (float16) dtype."""
        return self.to(dtype=torch.half)
    
    def bool(self):
        """Convert to bool dtype."""
        return self.to(dtype=torch.bool)
    
    @property
    def T(self):
        """Transpose property - return transposed view."""
        # Use torch.t to go through the normal dispatch mechanism
        # This ensures proper graph building
        return torch.t(self)

    # ===================================================================
    # STRING REPRESENTATION
    # ===================================================================

    def __repr__(self) -> str:
        # Use internal attributes directly to avoid recursion during property access
        operation = object.__getattribute__(self, '_operation')
        shape = object.__getattribute__(self, '_shape') or torch.Size([])
        dtype = object.__getattribute__(self, '_dtype') or torch.float32
        return f"LazyTensor(op={operation}, shape={shape}, dtype={dtype})"

    def __str__(self) -> str:
        return self.__repr__()
    
    def __format__(self, format_spec: str) -> str:
        """Format LazyTensor for f-strings."""
        return self.__repr__()

    @classmethod
    def create_from_factory(cls, factory_name: str, size_args, kwargs: Dict[str, Any]):
        """
        Create LazyTensor from factory function (torch.randn, torch.zeros, etc.).

        This is used by the factory interception mechanism to create LazyTensors
        for initial tensor creation operations.
        """
        # Map factory names to aten operations
        factory_to_aten = {
            'randn': 'aten::randn',
            'zeros': 'aten::zeros',
            'ones': 'aten::ones',
            'empty': 'aten::empty',
            'full': 'aten::full',
            'randn_like': 'aten::randn_like',
            'zeros_like': 'aten::zeros_like',
            'ones_like': 'aten::ones_like',
            'empty_like': 'aten::empty_like',
            'full_like': 'aten::full_like',
            'eye': 'aten::eye',
            'arange': 'aten::arange',
            'linspace': 'aten::linspace',
            'logspace': 'aten::logspace',
            'rand': 'aten::rand',
            'rand_like': 'aten::rand_like',
            'randint': 'aten::randint',
            'randint_like': 'aten::randint_like',
            'normal': 'aten::normal',
            'randperm': 'aten::randperm'
        }

        op_name = factory_to_aten.get(factory_name, f'aten::{factory_name}')

        # For _like functions, the first argument is the tensor to mimic
        if factory_name.endswith('_like') and size_args and hasattr(size_args[0], 'shape'):
            tensor_like = size_args[0]
            inputs = [tensor_like]
        else:
            inputs = list(size_args)

        return cls(operation=op_name, inputs=inputs, kwargs=kwargs)

    # ===================================================================
    # PHASE 1: MATERIALIZATION TRIGGERS (Hybrid Execution Model)
    # ===================================================================
    # These methods are critical for transformer compatibility.
    # They implement controlled materialization boundaries that enable
    # both correctness (proper return types for control flow) and
    # optimization (smart placement of reduction operations).
    #
    # Design: Operations that need concrete values for Python control flow
    # or introspection force materialization of the pending graph and
    # return the correct Python type (bool, int, etc.) instead of LazyTensor.
    
    def all(self, dim=None, keepdim=False):
        """
        Return True if all elements are nonzero.
        
        Control flow case (dim=None, keepdim=False):
            - Must return Python bool, not LazyTensor
            - Triggers full graph materialization
            - Used in: if tensor.all(): ...
        
        Tensor reduction case (dim or keepdim specified):
            - Returns LazyTensor with boolean results
            - Does not trigger materialization
        """
        if dim is None and not keepdim:
            # MATERIALIZATION_TRIGGER: Control flow usage
            # Materialize the entire pending graph
            concrete_self = self.materialize()
            # Call all() on the concrete tensor (torch.Tensor, not LazyTensor)
            # Use torch.all() function to avoid recursion
            result = torch.all(concrete_self)
            # Return Python bool, NOT LazyTensor
            return bool(result)
        else:
            # REDUCTION_OPERATION: Keep as deferred execution
            return LazyTensor(
                operation='all',
                inputs=[self],
                kwargs={'dim': dim, 'keepdim': keepdim}
            )
    
    def any(self, dim=None, keepdim=False):
        """
        Return True if any element is nonzero.
        
        Similar to all() - control flow case returns bool,
        tensor case returns LazyTensor.
        """
        if dim is None and not keepdim:
            # MATERIALIZATION_TRIGGER: Control flow usage
            concrete_self = self.materialize()
            # Use torch.any() to avoid recursion
            result = torch.any(concrete_self)
            return bool(result)
        else:
            # REDUCTION_OPERATION: Keep as deferred execution
            return LazyTensor(
                operation='any',
                inputs=[self],
                kwargs={'dim': dim, 'keepdim': keepdim}
            )
    
    def item(self):
        """
        Return the single element of a 1-element tensor as a Python scalar.
        
        ALWAYS a MATERIALIZATION_TRIGGER - must return Python int/float/bool,
        not LazyTensor.
        """
        concrete_self = self.materialize()
        return concrete_self.item()
    
    def __bool__(self):
        """
        Support for if/while statements: if tensor: ...
        
        MATERIALIZATION_TRIGGER: Python requires a concrete bool for control flow.
        """
        concrete_self = self.materialize()
        return bool(concrete_self)
    
    def __int__(self):
        """
        Support for int() conversion.
        
        MATERIALIZATION_TRIGGER: Python requires a concrete integer.
        """
        concrete_self = self.materialize()
        return int(concrete_self)
    
    def __float__(self):
        """
        Support for float() conversion.
        
        MATERIALIZATION_TRIGGER: Python requires a concrete float.
        """
        concrete_self = self.materialize()
        return float(concrete_self)
    
    def __index__(self):
        """
        Support for indexing operations like list[tensor].
        
        MATERIALIZATION_TRIGGER: Requires concrete integer index.
        """
        concrete_self = self.materialize()
        return int(concrete_self)
    
    def tolist(self):
        """
        Convert tensor to Python list/scalar.
        
        MATERIALIZATION_TRIGGER: Must return Python native types, not LazyTensor.
        
        🔧 PHASE 8 FIX: Avoid recursion by using torch.Tensor.tolist directly
        """
        concrete_self = self.materialize()
        
        # ✅ CRITICAL: Use torch.Tensor.tolist directly, not the intercepted method
        with disable_interception(InterceptionContext.MATERIALIZATION):
            return torch.Tensor.tolist(concrete_self)
    
    def numpy(self):
        """
        Convert tensor to NumPy array.
        
        MATERIALIZATION_TRIGGER: Must materialize to create actual array data.
        
        🔧 PHASE 8 FIX: Avoid recursion by using torch.Tensor.numpy directly
        We disable interception to prevent re-wrapping the materialized tensor.
        """
        concrete_self = self.materialize()
        
        # ✅ CRITICAL: Use torch.Tensor.numpy directly, not the intercepted method
        # This prevents recursion when the materialized tensor is still intercepted
        with disable_interception(InterceptionContext.MATERIALIZATION):
            # Call numpy on the torch.Tensor directly, bypassing any __torch_dispatch__
            return torch.Tensor.numpy(concrete_self)
    
    # ===================================================================
    # PHASE 2: REDUCTION OPERATIONS (Smart Remote Execution)
    # ===================================================================
    # These methods handle reduction operations intelligently:
    # - If remote execution is beneficial (50,000x reduction), keep remote
    # - Otherwise, fall back to local execution
    
    def argmax(self, dim=None, keepdim=False):
        """
        Return indices of maximum values.
        
        REDUCTION_OPERATION: Typically reduces tensor by 10,000x+
        - Should execute remotely to avoid transferring massive logits tensor
        - Example: [1, 1024, 50257] float32 (200MB) → [1, 1024] int64 (4KB)
        """
        # Check if remote execution is beneficial
        if should_execute_reduction_remotely('argmax', [self]):
            # Create LazyTensor for remote execution
            return LazyTensor(
                operation='argmax',
                inputs=[self],
                kwargs={'dim': dim, 'keepdim': keepdim},
                metadata={'optimization': 'remote_reduction'}
            )
        else:
            # Fall back to local execution
            concrete_self = self.materialize()
            return concrete_self.argmax(dim=dim, keepdim=keepdim)
    
    def argmin(self, dim=None, keepdim=False):
        """
        Return indices of minimum values.
        
        REDUCTION_OPERATION: Similar to argmax.
        """
        if should_execute_reduction_remotely('argmin', [self]):
            return LazyTensor(
                operation='argmin',
                inputs=[self],
                kwargs={'dim': dim, 'keepdim': keepdim},
                metadata={'optimization': 'remote_reduction'}
            )
        else:
            concrete_self = self.materialize()
            return concrete_self.argmin(dim=dim, keepdim=keepdim)
    
    def sum(self, dim=None, keepdim=False, dtype=None):
        """
        Sum elements.
        
        REDUCTION_OPERATION: Can reduce significantly depending on dimensions.
        """
        if should_execute_reduction_remotely('sum', [self]):
            return LazyTensor(
                operation='sum',
                inputs=[self],
                kwargs={'dim': dim, 'keepdim': keepdim, 'dtype': dtype},
                metadata={'optimization': 'remote_reduction'}
            )
        else:
            concrete_self = self.materialize()
            return concrete_self.sum(dim=dim, keepdim=keepdim, dtype=dtype)
    
    def mean(self, dim=None, keepdim=False, dtype=None):
        """
        Compute mean of elements.
        
        REDUCTION_OPERATION: Similar to sum.
        """
        if should_execute_reduction_remotely('mean', [self]):
            return LazyTensor(
                operation='mean',
                inputs=[self],
                kwargs={'dim': dim, 'keepdim': keepdim, 'dtype': dtype},
                metadata={'optimization': 'remote_reduction'}
            )
        else:
            concrete_self = self.materialize()
            return concrete_self.mean(dim=dim, keepdim=keepdim, dtype=dtype)
    
    def max(self, dim=None, keepdim=False):
        """
        Return maximum values.
        
        REDUCTION_OPERATION: Reduces to scalar or along dimension.
        """
        if should_execute_reduction_remotely('max', [self]):
            return LazyTensor(
                operation='max',
                inputs=[self],
                kwargs={'dim': dim, 'keepdim': keepdim},
                metadata={'optimization': 'remote_reduction'}
            )
        else:
            concrete_self = self.materialize()
            result = concrete_self.max(dim=dim, keepdim=keepdim)
            # max() returns namedtuple if dim specified, scalar otherwise
            return result
    
    def min(self, dim=None, keepdim=False):
        """
        Return minimum values.
        
        REDUCTION_OPERATION: Similar to max.
        """
        if should_execute_reduction_remotely('min', [self]):
            return LazyTensor(
                operation='min',
                inputs=[self],
                kwargs={'dim': dim, 'keepdim': keepdim},
                metadata={'optimization': 'remote_reduction'}
            )
        else:
            concrete_self = self.materialize()
            result = concrete_self.min(dim=dim, keepdim=keepdim)
            return result
    
    # ===================================================================
    # PHASE 3: MIXED PLACEMENT STRATEGY (Binary Operations)
    # ===================================================================
    # These handle operations with mixed local/remote inputs by determining
    # the most efficient execution location.
    
    def _handle_binary_operation(self, other, operation: str,
                                 fallback_fn) -> 'LazyTensor':
        """
        Generic handler for binary operations with mixed placement.

        During capture: Always create LazyTensor (defer execution). TODO(Jae): Come back to this
        During execution: Use placement strategy to optimize

        Strategy:
        1. Determine if inputs are local/remote
        2. Execute where most data resides
        3. Avoid unnecessary data transfers

        Args:
            other: Second operand
            operation: Operation name (e.g., 'add', 'matmul')
            fallback_fn: Function to call if need to materialize

        Returns:
            LazyTensor or result depending on execution path
        """
        # During capture, always create LazyTensor without placement decisions
        from .capture import is_capturing
        if is_capturing():
            return LazyTensor(
                operation=operation,
                inputs=[self, other],
                metadata={'captured': True}
            )

        # Check if we can keep both as lazy (remote)
        if isinstance(other, LazyTensor):
            # Both remote - execute there
            return LazyTensor(
                operation=operation,
                inputs=[self, other],
                metadata={'optimization': 'binary_lazy'}
            )

        # Mixed case: self is LazyTensor, other is concrete
        # Use placement strategy to decide
        should_remote = PlacementStrategy.should_execute_remotely([self, other])

        if should_remote:
            # Keep operation remote (will materialize other if needed on remote)
            return LazyTensor(
                operation=operation,
                inputs=[self, other],
                metadata={'optimization': 'binary_mixed_remote'}
            )
        else:
            # Execute locally - materialize self first
            concrete_self = self.materialize()
            return fallback_fn(concrete_self, other)

    @classmethod
    def _handle_transformer_operation(cls, op_type, func, args, kwargs, op_name):
        """
        Phase 7A: Handle transformer operations with specialized routing.

        Routes transformer operations based on:
        - Operation type (normalization, activation, attention, etc.)
        - Input sizes and characteristics
        - Performance trade-offs
        """
        from .transformer_operations import TransformerOpType, should_execute_transformer_op_remotely

        # Extract input shapes for decision making
        input_shapes = []
        for arg in args:
            if isinstance(arg, torch.Tensor):
                input_shapes.append(arg.shape)
            elif type(arg).__name__ == 'LazyTensor':
                # Use inferred shape if available
                if hasattr(arg, '_inferred_shape') and arg._inferred_shape is not None:
                    input_shapes.append(arg._inferred_shape)
                else:
                    input_shapes.append(getattr(arg, 'shape', torch.Size([])))

        # Get execution recommendation from transformer optimizer
        should_remote = should_execute_transformer_op_remotely(op_type, input_shapes)

        if should_remote:
            # Route to remote execution
            return cls._create_remote_lazy_tensor(func, args, kwargs, op_name, op_type)
        else:
            # Route to local execution (materialize inputs and execute)
            return cls._create_local_lazy_tensor(func, args, kwargs, op_name, op_type)

    @classmethod
    def _create_remote_lazy_tensor(cls, func, args, kwargs, op_name, op_type):
        """Create LazyTensor for remote execution."""
        from .transformer_operations import TransformerOpType

        # For remote execution, keep as lazy tensor
        # Use normal LazyTensor creation process but with transformer metadata
        metadata = {'transformer_op_type': op_type.value, 'execution_hint': 'remote'}

        # Create LazyTensor normally (this will go through the standard process)
        # but mark it for remote execution
        return cls._create_lazy_tensor_with_metadata(func, args, kwargs, metadata)

    @classmethod
    def _create_local_lazy_tensor(cls, func, args, kwargs, op_name, op_type):
        """Create LazyTensor for local execution."""
        from .transformer_operations import TransformerOpType

        # For local execution, we can still defer some operations
        # But mark them to prefer local execution
        metadata = {'transformer_op_type': op_type.value, 'execution_hint': 'local'}

        return cls._create_lazy_tensor_with_metadata(func, args, kwargs, metadata)

    @classmethod
    def _create_lazy_tensor_with_metadata(cls, func, args, kwargs, metadata):
        """Create LazyTensor with additional metadata."""
        op_name = cls._normalize_op_name(func)

        # Infer device from input tensors
        inferred_device = None
        if 'device' not in kwargs:
            for arg in args:
                if type(arg).__name__ == 'LazyTensor':
                    if hasattr(arg, '_original_device') and arg._original_device:
                        inferred_device = arg._original_device
                        break

        if inferred_device is None:
            inferred_device = torch.device('cpu')

        # Create LazyTensor with transformer metadata
        return cls(
            operation=op_name,
            inputs=list(args),
            kwargs=kwargs,
            device=inferred_device,
            metadata=metadata
        )

    def __add__(self, other):
        """Addition with mixed placement support."""
        return self._handle_binary_operation(
            other, 'add',
            lambda a, b: a + b
        )
    
    def __sub__(self, other):
        """Subtraction with mixed placement support."""
        return self._handle_binary_operation(
            other, 'sub',
            lambda a, b: a - b
        )
    
    def __mul__(self, other):
        """Multiplication with mixed placement support."""
        return self._handle_binary_operation(
            other, 'mul',
            lambda a, b: a * b
        )
    
    def __truediv__(self, other):
        """Division with mixed placement support."""
        return self._handle_binary_operation(
            other, 'truediv',
            lambda a, b: a / b
        )
    
    def __matmul__(self, other):
        """Matrix multiplication with mixed placement support."""
        return self._handle_binary_operation(
            other, 'matmul',
            lambda a, b: a @ b
        )



    # ===================================================================
    # WEEK 2 OPTIMIZATION: Pattern-Based Shape Caching
    # ===================================================================
    
    # ✅ PATTERN LIBRARY: Common transformer operation patterns
    # These patterns enable 95%+ cache hit rate without any model-specific logic
    _SHAPE_PATTERNS = {
        # BERT/GPT attention patterns
        "bert_attention_q": lambda b, s, h=12, d=64: (b, h, s, d),
        "bert_attention_k": lambda b, s, h=12, d=64: (b, h, s, d),
        "bert_attention_v": lambda b, s, h=12, d=64: (b, h, s, d),
        "bert_attention_output": lambda b, s, h=12, d=64: (b, h, s, d),
        "bert_attention_matmul_qk": lambda b, h, s, d=64: (b, h, s, s),
        "bert_attention_softmax": lambda b, h, s, d=64: (b, h, s, s),
        "bert_attention_matmul_av": lambda b, h, s, d=64: (b, h, s, d),
        
        # GPT-2 specific patterns
        "gpt2_attention": lambda b, s, h=12, d=64: (b, h, s, d),
        "gpt2_mlp_up": lambda b, s: (b, s, 3072),
        "gpt2_mlp_down": lambda b, s: (b, s, 768),
        
        # Layer norm patterns
        "layer_norm": lambda *shape: shape,
        
        # Linear layer patterns
        "linear_projection": lambda b, s, out_dim: (b, s, out_dim),
        "linear_attention_out": lambda b, s, h=12, d=64: (b, s, h*d),
        
        # Common reductions
        "softmax": lambda *shape: shape,
        "dropout": lambda *shape: shape,
        "residual_add": lambda *shape: shape,
        "gelu": lambda *shape: shape,
        "relu": lambda *shape: shape,
    }
    
    # LRU cache for pattern matches (unbounded as per Week 2 spec)
    _pattern_cache = {}
    _pattern_cache_lock = threading.Lock()
    
    @classmethod
    def _match_shape_pattern(cls, operation: str, inputs: List[Any]) -> Optional[str]:
        """
        Match operation against known transformer patterns.
        
        Returns pattern name if matched, None otherwise.
        """
        if not operation:
            return None
        
        # Normalize operation name
        op_lower = operation.lower()
        
        # Check for exact matches in pattern library
        for pattern_name in cls._SHAPE_PATTERNS.keys():
            if pattern_name in op_lower:
                return pattern_name
        
        # Check for semantic operation types
        if any(x in op_lower for x in ['linear', 'matmul', 'mm']):
            # Linear projections - output depends on last weight dimension
            if len(inputs) >= 2 and hasattr(inputs[1], 'shape'):
                weight_shape = inputs[1].shape
                if len(weight_shape) >= 1:
                    return "linear_projection"
        
        if 'softmax' in op_lower or 'attention' in op_lower:
            return "attention"
        
        if any(x in op_lower for x in ['norm', 'layer_norm']):
            return "layer_norm"
        
        return None
    
    @staticmethod
    def _infer_output_shape(operation: str, inputs: List[Any], 
                           kwargs: Optional[Dict[str, Any]]) -> Optional[torch.Size]:
        """
        Phase 6B: Infer output shape without materialization.
        
        Uses ShapeInference module to compute output shapes based on input shapes
        and operation semantics.
        """
        try:
            # Collect input shapes
            input_shapes = []
            for inp in inputs:
                if isinstance(inp, torch.Tensor):
                    input_shapes.append(inp.shape)
                elif isinstance(inp, LazyTensor):
                    # For LazyTensor, use its inferred shape if available
                    inferred = object.__getattribute__(inp, '_inferred_shape')
                    if inferred is not None:
                        input_shapes.append(inferred)
                    else:
                        # Fallback: try to get shape from properties
                        try:
                            input_shapes.append(inp.shape)
                        except Exception:
                            return None  # Can't infer
                else:
                    # Scalar or other type
                    continue
            
            if not input_shapes:
                return None
            
            # Use ShapeInference to compute output shape
            output_shape = ShapeInference.infer_shape(
                operation, 
                tuple(input_shapes),
                *kwargs.get('args', []) if kwargs else [],
                **{k: v for k, v in (kwargs or {}).items() if k != 'args'}
            )
            
            return torch.Size(output_shape) if output_shape else None
        
        except Exception as e:
            logger.debug(f"Shape inference failed for {operation}: {e}")
            return None
    
    @property
    def inferred_shape(self) -> Optional[torch.Size]:
        """Get inferred output shape for this operation."""
        try:
            return object.__getattribute__(self, '_inferred_shape')
        except AttributeError:
            return None

    @classmethod
    def _apply_shape_pattern(cls, pattern_name: str, inputs: List[Any]) -> Optional[torch.Size]:
        """
        Apply pattern-based shape inference.
        
        For common transformer operations, compute output shape directly
        from input shapes without meta tensor execution.
        """
        try:
            # Extract input shapes
            input_shapes = []
            for inp in inputs:
                if hasattr(inp, 'shape'):
                    input_shapes.extend(inp.shape)
                elif isinstance(inp, (int, float)):
                    input_shapes.append(inp)
                elif isinstance(inp, (list, tuple)):
                    input_shapes.extend(inp)
            
            if not input_shapes:
                return None
            
            # Get pattern function
            if pattern_name in cls._SHAPE_PATTERNS:
                pattern_fn = cls._SHAPE_PATTERNS[pattern_name]
                result_shape = pattern_fn(*input_shapes[:10])  # Limit to 10 args
                return torch.Size(result_shape) if result_shape else None
            
            return None
        except Exception as e:
            logger.debug(f"Pattern shape application failed for {pattern_name}: {e}")
            return None