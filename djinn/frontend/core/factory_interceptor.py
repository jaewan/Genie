"""
Factory function interceptor for tensor creation operations.

This module wraps torch.randn, torch.zeros, etc. to return LazyTensors
when device='remote_accelerator' is specified OR inside capture context.

Why necessary: Factory functions don't have LazyTensor arguments, so
__torch_dispatch__ won't be called. We need explicit namespace wrapping.

Performance: ~1-2μs overhead per creation, negligible for typical workloads.

Example:
    >>> interceptor = FactoryInterceptor()
    >>> interceptor.wrap()
    >>> with genie.capture():
    ...     x = torch.randn(10, 10)  # Returns LazyTensor
    >>> interceptor.unwrap()
"""

import torch
import functools
import logging
import threading
from typing import Any, Callable, Dict, Optional

logger = logging.getLogger(__name__)

# Import MetadataPlaceholder at module level to avoid per-call import overhead
from ...core.metadata import MetadataPlaceholder

# Thread-local storage to track when we're inside a LazyTensor factory method
# This prevents infinite recursion when factory interceptor calls LazyTensor methods
_in_lazy_factory = threading.local()

# Module-level import (avoids hot path import)
# Note: Executor is in djinn.server.executor, not djinn.frontend.core.executor
_executor_module = None
try:
    from ...server import executor as _executor_module
except ImportError:
    try:
        # Fallback: try relative import
        from ..server import executor as _executor_module
    except ImportError:
        pass


class FactoryInterceptor:
    """
    Intercepts PyTorch tensor creation functions.

    This module wraps torch.randn, torch.zeros, etc. to return LazyTensors
    when device='remote_accelerator' is specified OR inside capture context.

    **Why necessary:** Factory functions don't have LazyTensor arguments, so
    __torch_dispatch__ won't be called. We need explicit namespace wrapping
    to intercept tensor creation operations.

    **Performance:** ~1-2μs overhead per creation, negligible for typical workloads.
    This overhead is amortized over model inference where hundreds of operations
    are performed.

    **Thread Safety:** Each thread maintains its own capture state via thread-local
    storage. Factory wrapping is thread-safe and doesn't interfere across threads.

    Example:
        >>> interceptor = FactoryInterceptor()
        >>> interceptor.wrap()
        >>>
        >>> # Device-based API (paper API)
        >>> x = torch.randn(10, 10, device='remote_accelerator:0')
        >>> isinstance(x, LazyTensor)  # True
        >>>
        >>> # Context-based API (convenience API)
        >>> with genie.capture():
        ...     y = torch.randn(10, 10)  # No device needed
        >>> isinstance(y, LazyTensor)  # True
        >>>
        >>> interceptor.unwrap()  # Restore for testing
    """

    # Functions to intercept (complete list)
    # Based on validation script: 43 functions tested, 24 currently intercepted
    # Added high/medium priority functions to improve coverage from 24 → 40 functions
    FACTORY_FUNCTIONS = [
        # Basic creation
        'randn', 'rand', 'randint', 'randn_like', 'rand_like', 'randint_like',
        'zeros', 'ones', 'empty', 'full',
        'zeros_like', 'ones_like', 'empty_like', 'full_like',

        # Data conversion
        'tensor', 'as_tensor', 'from_numpy',

        # Special constructors
        'eye', 'arange', 'linspace', 'logspace',

        # Random distributions
        'normal', 'randperm',
        # ✅ ADDED: Advanced random distributions (medium priority)
        'bernoulli', 'multinomial', 'poisson',

        # ✅ ADDED: Matrix operations (high priority - commonly used)
        'diag', 'diagflat', 'tril', 'triu', 'vander',

        # ✅ ADDED: Shape manipulation (high priority)
        'atleast_1d', 'atleast_2d', 'atleast_3d',

        # ✅ ADDED: Grid generation (medium priority)
        'meshgrid', 'cartesian_prod',

        # ✅ ADDED: Complex numbers (medium priority)
        'complex', 'polar',

        # ✅ ADDED: Special functions (medium priority)
        'heaviside',

        # Advanced memory layout
        'empty_strided',
        
        # ⏸️  DEFERRED: PyTorch 2.0+ window functions (have compatibility errors)
        # 'asarray', 'kaiser_window', 'hann_window', 'hamming_window',
        # 'bartlett_window', 'blackman_window', 'frombuffer'
    ]

    def __init__(self):
        self._wrapped = False
        self._original_functions: Dict[str, Callable] = {}

    def wrap(self):
        """Wrap all factory functions in torch namespace."""
        if self._wrapped:
            logger.warning("Factory functions already wrapped")
            return

        for func_name in self.FACTORY_FUNCTIONS:
            if not hasattr(torch, func_name):
                logger.debug(f"torch.{func_name} not found, skipping")
                continue

            # Store original
            self._original_functions[func_name] = getattr(torch, func_name)

            # Create wrapper
            wrapper = self._create_wrapper(func_name)

            # Replace in torch namespace
            setattr(torch, func_name, wrapper)

        self._wrapped = True
        logger.info(f"Wrapped {len(self._original_functions)} factory functions")

    def unwrap(self):
        """Restore original factory functions (for testing)."""
        if not self._wrapped:
            return

        for func_name, original_func in self._original_functions.items():
            setattr(torch, func_name, original_func)

        self._wrapped = False
        self._original_functions.clear()

    def _create_wrapper(self, func_name: str) -> Callable:
        """Create wrapper function for a factory function."""
        original_func = self._original_functions[func_name]

        @functools.wraps(original_func)
        def wrapper(*args, **kwargs):
            device = kwargs.get('device')
            
            # ✅ FIX: For arange, extract scalar values from tensor arguments
            # torch.arange always returns a 1D tensor, even when given scalar tensors
            # We need to normalize arguments before creating LazyTensor or calling original
            if func_name == 'arange':
                normalized_args = []
                for arg in args:
                    if isinstance(arg, torch.Tensor) and arg.numel() == 1:
                        # Extract scalar value from tensor
                        normalized_args.append(arg.item())
                    elif hasattr(arg, 'numel') and hasattr(arg, 'item') and arg.numel() == 1:
                        # LazyTensor or similar - extract value
                        try:
                            normalized_args.append(arg.item())
                        except:
                            normalized_args.append(arg)
                    else:
                        normalized_args.append(arg)
                args = tuple(normalized_args)

            # CRITICAL FIX: Don't intercept meta or cpu devices UNLESS in capture mode TODO(Jae): Come back to this
            # These are used internally by PyTorch and LazyTensor for shape inference
            # But during capture, we want to intercept CPU tensors to create LazyTensors
            from .capture import is_capturing
            is_cpu_or_meta = (device is not None and
                (device == 'meta' or device == 'cpu' or
                 (isinstance(device, torch.device) and device.type in ('meta', 'cpu'))))

            if is_cpu_or_meta and not is_capturing():
                # Convert torch.device to string for original function
                if isinstance(device, torch.device):
                    fixed_kwargs = kwargs.copy()
                    fixed_kwargs['device'] = device.type
                    return original_func(*args, **fixed_kwargs)
                else:
                    return original_func(*args, **kwargs)
            # ✅ CRITICAL FIX: Check if interception should be disabled
            # This respects disable_interception(MATERIALIZATION) context
            from .interception_control import should_intercept
            if not should_intercept(device=device):
                # Interception disabled - call original function directly
                return original_func(*args, **kwargs)
            
            # Check if we're inside executor materialization (skip interception in that case)
            if _executor_module:
                executor_active = getattr(_executor_module._in_executor, 'active', False)
                if executor_active:
                    # Inside executor - don't return LazyTensor, create concrete tensor
                    # For remote devices, create on CPU instead
                    if self._is_remote_device(device):
                        # Create the tensor on CPU instead of remote device
                        materialized_kwargs = kwargs.copy()
                        materialized_kwargs['device'] = 'cpu'
                        return original_func(*args, **materialized_kwargs)
                    else:
                        # For other devices, call original function
                        return original_func(*args, **kwargs)

            # Check if we're already inside a LazyTensor factory method (prevent recursion)
            if getattr(_in_lazy_factory, 'active', False):
                # For remote devices, we need to handle them specially to avoid torch errors
                if device is not None and ('remote_accelerator' in str(device) or 'privateuseone' in str(device)):
                    # Return a placeholder - the LazyTensor constructor will handle this properly
                    return torch.empty(1, device='meta')  # This will be replaced
                # For meta devices during LazyTensor creation, don't intercept
                if device == 'meta' or (isinstance(device, torch.device) and device.type == 'meta'):
                    return original_func(*args, **kwargs)
                return original_func(*args, **kwargs)

            # Check if should return LazyTensor
            should_create_lazy = self._is_remote_device(device) or is_capturing()
            if should_create_lazy:
                from .lazy_tensor import LazyTensor

                # ✅ TRIGGER ASYNC INIT: First Djinn API call
                # This is one of the earliest points where Djinn code is invoked
                # Initialize runtime on first remote tensor creation or capture
                from ...backend.runtime.initialization import _ensure_async_init
                _ensure_async_init()

                # Set flag to prevent recursion
                _in_lazy_factory.active = True
                try:
                    # Call LazyTensor factory method (FIX: Simplified - factory methods now handle size conversion)
                    lazy_factory = getattr(LazyTensor, func_name, None)
                    if lazy_factory is not None:
                        return lazy_factory(*args, **kwargs)
                    else:
                        # Final fallback: generic LazyTensor creation with LAZY metadata
                        # ✅ FUNDAMENTAL FIX: Use MetadataPlaceholder for deferred semantic analysis
                        metadata = MetadataPlaceholder(
                            operation=f'aten::{func_name}',
                            inputs=tuple(args),
                            kwargs=kwargs
                        )
                        return LazyTensor(
                            operation=f'aten::{func_name}',
                            inputs=list(args),
                            kwargs=kwargs,
                            metadata=metadata  # ← Lazy metadata, not computed yet!
                        )
                finally:
                    # Always clear the flag
                    _in_lazy_factory.active = False
            # ✅ Only reached if NOT capturing AND NOT remote device
            return original_func(*args, **kwargs)

        return wrapper

    @staticmethod
    def _is_remote_device(device: Any) -> bool:
        """Check if device is remote_accelerator."""
        if device is None:
            return False

        device_str = str(device)
        return ('remote_accelerator' in device_str or
                'privateuseone' in device_str)


# Global interceptor instance
_factory_interceptor = FactoryInterceptor()


def wrap_factories():
    """Wrap factory functions (call once at initialization)."""
    _factory_interceptor.wrap()


def unwrap_factories():
    """Unwrap factory functions (for testing)."""
    _factory_interceptor.unwrap()


def get_factory_interceptor() -> FactoryInterceptor:
    """Get the global factory interceptor instance."""
    return _factory_interceptor